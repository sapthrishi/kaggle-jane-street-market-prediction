{"cells":[{"metadata":{"execution":{"iopub.execute_input":"2021-01-22T17:55:45.967945Z","iopub.status.busy":"2021-01-22T17:55:45.967126Z","iopub.status.idle":"2021-01-22T17:55:46.016606Z","shell.execute_reply":"2021-01-22T17:55:46.015938Z"},"papermill":{"duration":0.080791,"end_time":"2021-01-22T17:55:46.016735","exception":false,"start_time":"2021-01-22T17:55:45.935944","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/jane-smooth-mynn5-auc-150x3/Jane_mynn5_auc_150_150_150.pth\n/kaggle/input/jane-street-market-prediction/example_sample_submission.csv\n/kaggle/input/jane-street-market-prediction/features.csv\n/kaggle/input/jane-street-market-prediction/example_test.csv\n/kaggle/input/jane-street-market-prediction/train.csv\n/kaggle/input/jane-street-market-prediction/janestreet/competition.cpython-37m-x86_64-linux-gnu.so\n/kaggle/input/jane-street-market-prediction/janestreet/__init__.py\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"THREE_HIDDEN_LAYERS = [150, 150, 150]","execution_count":2,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-22T17:55:46.078797Z","iopub.status.busy":"2021-01-22T17:55:46.077856Z","iopub.status.idle":"2021-01-22T17:55:58.435764Z","shell.execute_reply":"2021-01-22T17:55:58.434712Z"},"papermill":{"duration":12.397325,"end_time":"2021-01-22T17:55:58.43594","exception":false,"start_time":"2021-01-22T17:55:46.038615","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import os\nimport gc\nimport pandas as pd\nimport numpy as np\n\nimport fastai\nfrom   fastai.callback import *\nfrom   fastai.callback.all import *\nfrom   fastai.callback.training import GradientClip\nfrom   fastai.callback.all import SaveModelCallback, EarlyStoppingCallback, ReduceLROnPlateau \nfrom   fastai.tabular import *\nfrom   fastai.tabular.data import *\nfrom   fastai.tabular.all import *\nfrom   fastai.tabular.all import TabularPandas, RandomSplitter, CategoryBlock, MultiCategoryBlock, range_of, accuracy, tabular_learner, TabularDataLoaders\n# from fastai import datasets\n# from fastai.dataset import ModelData,ArraysIndexDataset\n# from fastai.dataloader import DataLoader\nfrom   fastai.learner import Learner\nfrom   fastai.metrics import RocAucMulti\n\nfrom   sklearn.pipeline import Pipeline\nfrom   sklearn.impute import SimpleImputer\nfrom   sklearn.preprocessing import StandardScaler\n\nimport torch.nn as nn\nfrom   torch.nn import CrossEntropyLoss, MSELoss\nfrom   torch.nn.modules.loss import _WeightedLoss\n\nfrom   functools import partial\nimport warnings\nwarnings.filterwarnings (\"ignore\")","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Global Vars\nTP   = None\nDF   = None\nDLs  = None\nPIPE = None\nBS   = 10000\nN_FEATURES  = 0\nN_FEAT_TAGS = 0","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtype = {\n    'feature'  : 'str', \n    'tag_0'    : 'int8'\n}\nfor i in range (1, 29):\n    k = 'tag_' + str (i)\n    dtype[k] = 'int8'\n    \nfeatures_df = pd.read_csv ('../input/jane-street-market-prediction/features.csv', usecols=range (1,30), dtype=dtype)\nN_FEATURES  = features_df.shape[0]  # the features.csv has 130 features (1st row) = no of features in train.csv (feature_0 to feature_129)\nN_FEAT_TAGS = features_df.shape[1]  # the features.csv has 29 tags\n\n# features_df.head ()\ndel features_df\ngc.collect ()\nN_FEATURES, N_FEAT_TAGS","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"(130, 29)"},"metadata":{}}]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-22T17:55:58.538074Z","iopub.status.busy":"2021-01-22T17:55:58.537413Z","iopub.status.idle":"2021-01-22T17:55:58.552729Z","shell.execute_reply":"2021-01-22T17:55:58.553283Z"},"papermill":{"duration":0.040645,"end_time":"2021-01-22T17:55:58.553426","exception":false,"start_time":"2021-01-22T17:55:58.512781","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def preprocess_data (filename='../input/jane-street-market-prediction/train.csv', df=None, isTrainData=True):\n    \n    global PIPE\n    dtype = None\n    if isTrainData:\n        \n        dtype = {\n            'date'      : 'int64', \n            'weight'    : 'float32',\n            'resp'      : 'float32',\n            'ts_id'     : 'int64',  \n            'feature_0' : 'float32'\n        }\n    else:\n        \n        dtype = {\n            'date'      : 'int64', \n            'weight'    : 'float32',\n            'feature_0' : 'float32'\n        }\n    for i in range (1, 130):\n        k = 'feature_' + str (i)\n        dtype[k] = 'float32'\n    \n    to   = None\n    if isTrainData:\n        df         = pd.read_csv (filename, dtype=dtype)\n        df         = df.query ('date > 85')\n        # df       = df[df['weight'] != 0].reset_index (drop = True)\n        df         = df.reset_index (drop = True)\n        \n        resp_cols  = ['resp_1', 'resp_2', 'resp_3','resp_4', 'resp']    \n        # df[:5000].to_csv (filename+'.dummy', index=False) \n        y          = np.stack ([(df[c] > 0).astype ('int') for c in resp_cols]).T\n        df.drop (columns=['weight', 'date', 'ts_id']+resp_cols, inplace=True)\n        f_columns  = [c for c in df.columns if \"feature\" in c]\n        PIPE       = Pipeline ([\n                        (\"imputer\", SimpleImputer (missing_values=np.nan, strategy='mean')),\n                        # (\"stand\",   StandardScaler (with_mean=False))\n        ])\n        columns    = list (df.columns) + resp_cols\n        X          = PIPE.fit_transform (df)\n        df         = pd.DataFrame (np.hstack ((X, y)))\n        df.columns = columns\n        del X, y\n\n        splits    = RandomSplitter (valid_pct=0.05) (range_of (df))\n        to        = TabularPandas (df, cont_names=f_columns, cat_names=None, y_names=resp_cols, y_block=MultiCategoryBlock(encoded=True, vocab=resp_cols), splits=splits)\n    else:\n        \n        df         = df.drop (columns=['weight', 'date']).reset_index (drop = True)\n        columns    = df.columns\n        X          = PIPE.transform (df)\n        df         = pd.DataFrame (X)\n        df.columns = columns\n        # del X\n    return to, df","execution_count":6,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-22T17:55:58.601049Z","iopub.status.busy":"2021-01-22T17:55:58.600425Z","iopub.status.idle":"2021-01-22T17:58:44.347053Z","shell.execute_reply":"2021-01-22T17:58:44.347603Z"},"papermill":{"duration":165.771921,"end_time":"2021-01-22T17:58:44.347759","exception":false,"start_time":"2021-01-22T17:55:58.575838","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"TP, DF = preprocess_data ()\nTP.xs.iloc[:2]","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"         feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n1486021        1.0   0.887128   0.589413  -0.365736  -0.287576  -0.259741   \n498170         1.0  -0.038337  -0.442419  -0.352489  -0.268943  -0.648557   \n\n         feature_6  feature_7  feature_8  feature_9  ...  feature_120  \\\n1486021  -0.212156   0.465504   0.378237   3.147892  ...     1.738644   \n498170   -0.509457   0.038918  -0.078841  -0.286071  ...    -0.644247   \n\n         feature_121  feature_122  feature_123  feature_124  feature_125  \\\n1486021     0.187857     2.907865     1.196368     2.071057     0.951451   \n498170     -1.025272    -0.594824    -0.538581    -0.484385    -0.721402   \n\n         feature_126  feature_127  feature_128  feature_129  \n1486021     3.404408     1.106853     3.172173     1.061541  \n498170     -0.539776    -0.185960    -0.486702    -0.202106  \n\n[2 rows x 130 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature_0</th>\n      <th>feature_1</th>\n      <th>feature_2</th>\n      <th>feature_3</th>\n      <th>feature_4</th>\n      <th>feature_5</th>\n      <th>feature_6</th>\n      <th>feature_7</th>\n      <th>feature_8</th>\n      <th>feature_9</th>\n      <th>...</th>\n      <th>feature_120</th>\n      <th>feature_121</th>\n      <th>feature_122</th>\n      <th>feature_123</th>\n      <th>feature_124</th>\n      <th>feature_125</th>\n      <th>feature_126</th>\n      <th>feature_127</th>\n      <th>feature_128</th>\n      <th>feature_129</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1486021</th>\n      <td>1.0</td>\n      <td>0.887128</td>\n      <td>0.589413</td>\n      <td>-0.365736</td>\n      <td>-0.287576</td>\n      <td>-0.259741</td>\n      <td>-0.212156</td>\n      <td>0.465504</td>\n      <td>0.378237</td>\n      <td>3.147892</td>\n      <td>...</td>\n      <td>1.738644</td>\n      <td>0.187857</td>\n      <td>2.907865</td>\n      <td>1.196368</td>\n      <td>2.071057</td>\n      <td>0.951451</td>\n      <td>3.404408</td>\n      <td>1.106853</td>\n      <td>3.172173</td>\n      <td>1.061541</td>\n    </tr>\n    <tr>\n      <th>498170</th>\n      <td>1.0</td>\n      <td>-0.038337</td>\n      <td>-0.442419</td>\n      <td>-0.352489</td>\n      <td>-0.268943</td>\n      <td>-0.648557</td>\n      <td>-0.509457</td>\n      <td>0.038918</td>\n      <td>-0.078841</td>\n      <td>-0.286071</td>\n      <td>...</td>\n      <td>-0.644247</td>\n      <td>-1.025272</td>\n      <td>-0.594824</td>\n      <td>-0.538581</td>\n      <td>-0.484385</td>\n      <td>-0.721402</td>\n      <td>-0.539776</td>\n      <td>-0.185960</td>\n      <td>-0.486702</td>\n      <td>-0.202106</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows Ã— 130 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"TP.ys.iloc[:2]","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"         resp_1  resp_2  resp_3  resp_4  resp\n1486021     0.0     0.0     0.0     0.0   0.0\n498170      0.0     0.0     0.0     1.0   1.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>resp_1</th>\n      <th>resp_2</th>\n      <th>resp_3</th>\n      <th>resp_4</th>\n      <th>resp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1486021</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>498170</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"TP.xs.shape, TP.ys.shape","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"((1862597, 130), (1862597, 5))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"DLs = TP.dataloaders (bs=BS)\nDLs.show_batch ()","execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature_0</th>\n      <th>feature_1</th>\n      <th>feature_2</th>\n      <th>feature_3</th>\n      <th>feature_4</th>\n      <th>feature_5</th>\n      <th>feature_6</th>\n      <th>feature_7</th>\n      <th>feature_8</th>\n      <th>feature_9</th>\n      <th>feature_10</th>\n      <th>feature_11</th>\n      <th>feature_12</th>\n      <th>feature_13</th>\n      <th>feature_14</th>\n      <th>feature_15</th>\n      <th>feature_16</th>\n      <th>feature_17</th>\n      <th>feature_18</th>\n      <th>feature_19</th>\n      <th>feature_20</th>\n      <th>feature_21</th>\n      <th>feature_22</th>\n      <th>feature_23</th>\n      <th>feature_24</th>\n      <th>feature_25</th>\n      <th>feature_26</th>\n      <th>feature_27</th>\n      <th>feature_28</th>\n      <th>feature_29</th>\n      <th>feature_30</th>\n      <th>feature_31</th>\n      <th>feature_32</th>\n      <th>feature_33</th>\n      <th>feature_34</th>\n      <th>feature_35</th>\n      <th>feature_36</th>\n      <th>feature_37</th>\n      <th>feature_38</th>\n      <th>feature_39</th>\n      <th>feature_40</th>\n      <th>feature_41</th>\n      <th>feature_42</th>\n      <th>feature_43</th>\n      <th>feature_44</th>\n      <th>feature_45</th>\n      <th>feature_46</th>\n      <th>feature_47</th>\n      <th>feature_48</th>\n      <th>feature_49</th>\n      <th>feature_50</th>\n      <th>feature_51</th>\n      <th>feature_52</th>\n      <th>feature_53</th>\n      <th>feature_54</th>\n      <th>feature_55</th>\n      <th>feature_56</th>\n      <th>feature_57</th>\n      <th>feature_58</th>\n      <th>feature_59</th>\n      <th>feature_60</th>\n      <th>feature_61</th>\n      <th>feature_62</th>\n      <th>feature_63</th>\n      <th>feature_64</th>\n      <th>feature_65</th>\n      <th>feature_66</th>\n      <th>feature_67</th>\n      <th>feature_68</th>\n      <th>feature_69</th>\n      <th>feature_70</th>\n      <th>feature_71</th>\n      <th>feature_72</th>\n      <th>feature_73</th>\n      <th>feature_74</th>\n      <th>feature_75</th>\n      <th>feature_76</th>\n      <th>feature_77</th>\n      <th>feature_78</th>\n      <th>feature_79</th>\n      <th>feature_80</th>\n      <th>feature_81</th>\n      <th>feature_82</th>\n      <th>feature_83</th>\n      <th>feature_84</th>\n      <th>feature_85</th>\n      <th>feature_86</th>\n      <th>feature_87</th>\n      <th>feature_88</th>\n      <th>feature_89</th>\n      <th>feature_90</th>\n      <th>feature_91</th>\n      <th>feature_92</th>\n      <th>feature_93</th>\n      <th>feature_94</th>\n      <th>feature_95</th>\n      <th>feature_96</th>\n      <th>feature_97</th>\n      <th>feature_98</th>\n      <th>feature_99</th>\n      <th>feature_100</th>\n      <th>feature_101</th>\n      <th>feature_102</th>\n      <th>feature_103</th>\n      <th>feature_104</th>\n      <th>feature_105</th>\n      <th>feature_106</th>\n      <th>feature_107</th>\n      <th>feature_108</th>\n      <th>feature_109</th>\n      <th>feature_110</th>\n      <th>feature_111</th>\n      <th>feature_112</th>\n      <th>feature_113</th>\n      <th>feature_114</th>\n      <th>feature_115</th>\n      <th>feature_116</th>\n      <th>feature_117</th>\n      <th>feature_118</th>\n      <th>feature_119</th>\n      <th>feature_120</th>\n      <th>feature_121</th>\n      <th>feature_122</th>\n      <th>feature_123</th>\n      <th>feature_124</th>\n      <th>feature_125</th>\n      <th>feature_126</th>\n      <th>feature_127</th>\n      <th>feature_128</th>\n      <th>feature_129</th>\n      <th>resp_1</th>\n      <th>resp_2</th>\n      <th>resp_3</th>\n      <th>resp_4</th>\n      <th>resp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>-1.402350</td>\n      <td>-1.465863</td>\n      <td>2.104110</td>\n      <td>2.240142</td>\n      <td>1.953950</td>\n      <td>2.114482</td>\n      <td>1.393767</td>\n      <td>1.840540</td>\n      <td>-1.038215</td>\n      <td>-0.731334</td>\n      <td>0.394429</td>\n      <td>0.725271</td>\n      <td>-0.518386</td>\n      <td>-0.459564</td>\n      <td>-0.973620</td>\n      <td>-1.364803</td>\n      <td>1.581157</td>\n      <td>2.443805</td>\n      <td>0.358889</td>\n      <td>0.421961</td>\n      <td>1.561455</td>\n      <td>1.171271</td>\n      <td>0.748929</td>\n      <td>0.975993</td>\n      <td>0.373093</td>\n      <td>0.393379</td>\n      <td>-0.027247</td>\n      <td>-0.034575</td>\n      <td>-0.310006</td>\n      <td>-0.371925</td>\n      <td>-0.071772</td>\n      <td>-0.105742</td>\n      <td>-0.164980</td>\n      <td>-0.151946</td>\n      <td>-0.383539</td>\n      <td>-0.324010</td>\n      <td>2.593501</td>\n      <td>3.278503</td>\n      <td>-0.004750</td>\n      <td>-0.004010</td>\n      <td>5.468422</td>\n      <td>-1.314294</td>\n      <td>-0.047890</td>\n      <td>-0.385999</td>\n      <td>0.517724</td>\n      <td>-0.220823</td>\n      <td>-0.714392</td>\n      <td>-0.600042</td>\n      <td>-0.464348</td>\n      <td>0.435525</td>\n      <td>2.813271</td>\n      <td>-0.153293</td>\n      <td>-0.769910</td>\n      <td>0.892870</td>\n      <td>5.424226</td>\n      <td>2.816256</td>\n      <td>6.485656</td>\n      <td>6.017932</td>\n      <td>4.641366</td>\n      <td>0.895211</td>\n      <td>0.781195</td>\n      <td>4.616206</td>\n      <td>4.734525</td>\n      <td>1.940365</td>\n      <td>2.580112</td>\n      <td>2.242616</td>\n      <td>-3.444567</td>\n      <td>-3.251457</td>\n      <td>-1.549777</td>\n      <td>-1.340071</td>\n      <td>-1.297101</td>\n      <td>-0.187862</td>\n      <td>-0.491190</td>\n      <td>0.121328</td>\n      <td>-0.198096</td>\n      <td>-0.282092</td>\n      <td>-0.664177</td>\n      <td>-0.890981</td>\n      <td>-0.932395</td>\n      <td>0.334995</td>\n      <td>-0.654620</td>\n      <td>-0.513697</td>\n      <td>-1.536232</td>\n      <td>-0.241900</td>\n      <td>-1.238212</td>\n      <td>-0.175811</td>\n      <td>-0.346747</td>\n      <td>-0.210041</td>\n      <td>-0.003387</td>\n      <td>0.788769</td>\n      <td>-1.515613</td>\n      <td>0.676564</td>\n      <td>0.583375</td>\n      <td>0.223276</td>\n      <td>1.573033</td>\n      <td>-0.187031</td>\n      <td>0.417197</td>\n      <td>-0.479648</td>\n      <td>-0.018889</td>\n      <td>0.143404</td>\n      <td>0.065426</td>\n      <td>1.152066</td>\n      <td>1.137767</td>\n      <td>0.366804</td>\n      <td>0.900154</td>\n      <td>1.306178</td>\n      <td>1.687094</td>\n      <td>-0.424409</td>\n      <td>-1.206583</td>\n      <td>-0.360831</td>\n      <td>-0.311924</td>\n      <td>-0.895708</td>\n      <td>0.024831</td>\n      <td>0.896987</td>\n      <td>0.130753</td>\n      <td>0.671858</td>\n      <td>1.024732</td>\n      <td>0.708943</td>\n      <td>1.273603</td>\n      <td>-1.150756</td>\n      <td>-0.297180</td>\n      <td>-1.026262</td>\n      <td>0.255678</td>\n      <td>-1.139591</td>\n      <td>-0.994713</td>\n      <td>-1.825980</td>\n      <td>-0.656015</td>\n      <td>-1.424490</td>\n      <td>-0.232307</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>-0.528750</td>\n      <td>-0.599703</td>\n      <td>1.042823</td>\n      <td>0.979929</td>\n      <td>-1.205378</td>\n      <td>-1.202112</td>\n      <td>-0.538931</td>\n      <td>-0.623659</td>\n      <td>-2.229858</td>\n      <td>-1.476747</td>\n      <td>0.008779</td>\n      <td>-0.049320</td>\n      <td>0.714197</td>\n      <td>0.652538</td>\n      <td>-2.159513</td>\n      <td>-2.827457</td>\n      <td>0.703596</td>\n      <td>0.969273</td>\n      <td>0.319995</td>\n      <td>0.335449</td>\n      <td>1.466262</td>\n      <td>1.032425</td>\n      <td>1.337713</td>\n      <td>1.803514</td>\n      <td>0.330246</td>\n      <td>0.307365</td>\n      <td>-1.089323</td>\n      <td>-1.769790</td>\n      <td>-0.353834</td>\n      <td>-0.410275</td>\n      <td>-0.840897</td>\n      <td>-1.607131</td>\n      <td>-1.516582</td>\n      <td>-1.662121</td>\n      <td>-2.848514</td>\n      <td>-2.783668</td>\n      <td>1.320821</td>\n      <td>1.514201</td>\n      <td>-0.422411</td>\n      <td>-0.710926</td>\n      <td>1.193790</td>\n      <td>-1.750477</td>\n      <td>0.102009</td>\n      <td>2.222414</td>\n      <td>4.886374</td>\n      <td>-1.288712</td>\n      <td>-2.472361</td>\n      <td>-2.192449</td>\n      <td>-1.712446</td>\n      <td>-1.565702</td>\n      <td>-1.773601</td>\n      <td>-0.153293</td>\n      <td>-2.458107</td>\n      <td>-1.515374</td>\n      <td>-1.200339</td>\n      <td>-0.017492</td>\n      <td>-1.371621</td>\n      <td>-1.326772</td>\n      <td>-0.473575</td>\n      <td>-0.729661</td>\n      <td>-0.617133</td>\n      <td>0.434335</td>\n      <td>0.451011</td>\n      <td>-1.616088</td>\n      <td>-0.317840</td>\n      <td>-0.237195</td>\n      <td>-1.313813</td>\n      <td>-1.264840</td>\n      <td>-1.616964</td>\n      <td>-0.744855</td>\n      <td>-1.371814</td>\n      <td>-0.832141</td>\n      <td>-0.299974</td>\n      <td>-0.513851</td>\n      <td>-0.336182</td>\n      <td>-0.168201</td>\n      <td>-1.497037</td>\n      <td>-3.903168</td>\n      <td>-1.011034</td>\n      <td>-2.200729</td>\n      <td>-1.863887</td>\n      <td>-0.598199</td>\n      <td>-4.225890</td>\n      <td>0.332346</td>\n      <td>-0.341405</td>\n      <td>-0.197238</td>\n      <td>-0.354246</td>\n      <td>-0.093689</td>\n      <td>-0.459872</td>\n      <td>3.267473</td>\n      <td>-0.186759</td>\n      <td>1.521631</td>\n      <td>1.506106</td>\n      <td>1.228351</td>\n      <td>2.078219</td>\n      <td>0.758645</td>\n      <td>0.177167</td>\n      <td>0.339511</td>\n      <td>0.109919</td>\n      <td>0.087396</td>\n      <td>-0.107591</td>\n      <td>4.877015</td>\n      <td>1.522875</td>\n      <td>2.524619</td>\n      <td>2.334671</td>\n      <td>2.458480</td>\n      <td>2.755153</td>\n      <td>0.813243</td>\n      <td>-1.487340</td>\n      <td>-0.023361</td>\n      <td>-0.227471</td>\n      <td>-0.883679</td>\n      <td>-0.360195</td>\n      <td>3.927306</td>\n      <td>0.838675</td>\n      <td>2.672040</td>\n      <td>2.611385</td>\n      <td>2.044884</td>\n      <td>1.945273</td>\n      <td>-1.490444</td>\n      <td>-0.091311</td>\n      <td>-1.504916</td>\n      <td>0.249750</td>\n      <td>-1.349767</td>\n      <td>-0.314882</td>\n      <td>-2.011311</td>\n      <td>0.220119</td>\n      <td>-1.514502</td>\n      <td>0.670862</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-1.0</td>\n      <td>2.441307</td>\n      <td>0.568084</td>\n      <td>-3.127625</td>\n      <td>-1.714037</td>\n      <td>-2.072506</td>\n      <td>-1.079726</td>\n      <td>0.157651</td>\n      <td>-0.174433</td>\n      <td>1.422109</td>\n      <td>-0.234213</td>\n      <td>0.935506</td>\n      <td>0.314981</td>\n      <td>1.366328</td>\n      <td>0.039008</td>\n      <td>1.575150</td>\n      <td>0.225683</td>\n      <td>-0.230976</td>\n      <td>-0.143529</td>\n      <td>-0.747016</td>\n      <td>-0.473459</td>\n      <td>-0.640116</td>\n      <td>-0.175276</td>\n      <td>-0.723687</td>\n      <td>-0.378981</td>\n      <td>-0.916185</td>\n      <td>-0.484297</td>\n      <td>0.763867</td>\n      <td>0.475456</td>\n      <td>1.906804</td>\n      <td>1.060871</td>\n      <td>1.347871</td>\n      <td>0.958390</td>\n      <td>1.795624</td>\n      <td>0.859777</td>\n      <td>3.125851</td>\n      <td>1.445059</td>\n      <td>-3.909073</td>\n      <td>-2.627216</td>\n      <td>0.988878</td>\n      <td>0.760562</td>\n      <td>0.513044</td>\n      <td>1.304083</td>\n      <td>3.734949</td>\n      <td>-2.990855</td>\n      <td>-0.431060</td>\n      <td>3.747401</td>\n      <td>3.391699</td>\n      <td>2.243201</td>\n      <td>0.920829</td>\n      <td>3.456962</td>\n      <td>1.774228</td>\n      <td>3.034981</td>\n      <td>0.310215</td>\n      <td>-0.918135</td>\n      <td>-0.236175</td>\n      <td>0.269684</td>\n      <td>-0.444327</td>\n      <td>-0.817238</td>\n      <td>0.379349</td>\n      <td>-0.118395</td>\n      <td>-0.105821</td>\n      <td>1.696237</td>\n      <td>1.756023</td>\n      <td>2.862614</td>\n      <td>4.011180</td>\n      <td>3.437301</td>\n      <td>3.125962</td>\n      <td>3.081427</td>\n      <td>2.111888</td>\n      <td>1.292331</td>\n      <td>0.229675</td>\n      <td>0.097236</td>\n      <td>0.447320</td>\n      <td>0.248956</td>\n      <td>0.382320</td>\n      <td>0.655741</td>\n      <td>-3.136839</td>\n      <td>0.335576</td>\n      <td>0.730718</td>\n      <td>0.674997</td>\n      <td>1.144335</td>\n      <td>1.178178</td>\n      <td>-5.416130</td>\n      <td>-1.392782</td>\n      <td>0.314047</td>\n      <td>-0.705325</td>\n      <td>-0.609087</td>\n      <td>0.367906</td>\n      <td>0.131215</td>\n      <td>-0.980159</td>\n      <td>1.235924</td>\n      <td>-0.380440</td>\n      <td>0.211646</td>\n      <td>1.128685</td>\n      <td>1.682559</td>\n      <td>-1.900745</td>\n      <td>-1.338859</td>\n      <td>-2.175540</td>\n      <td>-1.257774</td>\n      <td>-1.194013</td>\n      <td>0.817891</td>\n      <td>-1.555740</td>\n      <td>-0.940190</td>\n      <td>-1.652230</td>\n      <td>-1.510224</td>\n      <td>-1.781693</td>\n      <td>2.779250</td>\n      <td>-3.425866</td>\n      <td>-1.474126</td>\n      <td>-1.471332</td>\n      <td>-1.075187</td>\n      <td>-1.244693</td>\n      <td>0.653631</td>\n      <td>-1.240557</td>\n      <td>-0.362941</td>\n      <td>-1.458179</td>\n      <td>-0.950783</td>\n      <td>0.146107</td>\n      <td>1.809439</td>\n      <td>0.551159</td>\n      <td>-1.169740</td>\n      <td>0.183437</td>\n      <td>-1.530307</td>\n      <td>0.515810</td>\n      <td>-1.337197</td>\n      <td>0.710297</td>\n      <td>-1.070090</td>\n      <td>0.483888</td>\n      <td>-1.154029</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-1.0</td>\n      <td>-2.392209</td>\n      <td>-2.222724</td>\n      <td>-1.787653</td>\n      <td>-2.556286</td>\n      <td>-0.772579</td>\n      <td>-1.117404</td>\n      <td>-0.277536</td>\n      <td>-0.292429</td>\n      <td>-1.708787</td>\n      <td>-0.880625</td>\n      <td>-0.239957</td>\n      <td>-0.178586</td>\n      <td>-0.925504</td>\n      <td>-0.610288</td>\n      <td>-1.034005</td>\n      <td>-1.074760</td>\n      <td>-0.057234</td>\n      <td>-0.095010</td>\n      <td>-0.194568</td>\n      <td>-0.302896</td>\n      <td>-0.104029</td>\n      <td>-0.117906</td>\n      <td>-0.135430</td>\n      <td>-0.213777</td>\n      <td>-0.190415</td>\n      <td>-0.289311</td>\n      <td>0.150603</td>\n      <td>0.339455</td>\n      <td>0.196560</td>\n      <td>0.476598</td>\n      <td>0.227959</td>\n      <td>0.545917</td>\n      <td>0.176412</td>\n      <td>0.318183</td>\n      <td>0.268436</td>\n      <td>0.481140</td>\n      <td>-2.020407</td>\n      <td>-3.338234</td>\n      <td>0.091973</td>\n      <td>0.222121</td>\n      <td>2.382938</td>\n      <td>-2.182102</td>\n      <td>-1.403707</td>\n      <td>2.885776</td>\n      <td>3.898218</td>\n      <td>-0.937645</td>\n      <td>-2.029636</td>\n      <td>-1.526200</td>\n      <td>-1.065102</td>\n      <td>-0.684920</td>\n      <td>0.051572</td>\n      <td>-1.093033</td>\n      <td>-3.103745</td>\n      <td>-1.900638</td>\n      <td>0.285571</td>\n      <td>-0.227310</td>\n      <td>-0.942310</td>\n      <td>-0.984270</td>\n      <td>-1.531301</td>\n      <td>-1.474198</td>\n      <td>-1.233971</td>\n      <td>4.115300</td>\n      <td>4.217609</td>\n      <td>1.981865</td>\n      <td>3.333439</td>\n      <td>2.871905</td>\n      <td>-0.552463</td>\n      <td>-0.530169</td>\n      <td>-2.026985</td>\n      <td>-1.282952</td>\n      <td>-1.303756</td>\n      <td>0.259123</td>\n      <td>0.612567</td>\n      <td>0.285846</td>\n      <td>0.354115</td>\n      <td>0.580378</td>\n      <td>-1.883076</td>\n      <td>1.075935</td>\n      <td>1.227207</td>\n      <td>0.861880</td>\n      <td>1.200489</td>\n      <td>1.170276</td>\n      <td>-3.659056</td>\n      <td>-0.466423</td>\n      <td>0.580226</td>\n      <td>-0.561026</td>\n      <td>-0.689799</td>\n      <td>0.239602</td>\n      <td>0.542459</td>\n      <td>0.579792</td>\n      <td>2.377130</td>\n      <td>0.041812</td>\n      <td>0.274364</td>\n      <td>1.117303</td>\n      <td>2.588760</td>\n      <td>-0.852205</td>\n      <td>-1.338859</td>\n      <td>-1.793604</td>\n      <td>-1.257774</td>\n      <td>-1.194013</td>\n      <td>0.754721</td>\n      <td>0.309720</td>\n      <td>-0.940190</td>\n      <td>-1.036199</td>\n      <td>-1.510224</td>\n      <td>-1.781693</td>\n      <td>3.054055</td>\n      <td>-1.200187</td>\n      <td>-1.037364</td>\n      <td>-1.189646</td>\n      <td>-1.129323</td>\n      <td>-1.435075</td>\n      <td>0.823085</td>\n      <td>0.418672</td>\n      <td>0.597974</td>\n      <td>-0.714921</td>\n      <td>-0.861275</td>\n      <td>0.133092</td>\n      <td>2.283215</td>\n      <td>-2.206041</td>\n      <td>-1.904351</td>\n      <td>-3.445347</td>\n      <td>-2.605718</td>\n      <td>-2.107215</td>\n      <td>-2.540299</td>\n      <td>-4.648353</td>\n      <td>-2.401599</td>\n      <td>-4.413234</td>\n      <td>-2.491531</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-1.0</td>\n      <td>0.788438</td>\n      <td>0.679424</td>\n      <td>2.920934</td>\n      <td>2.876919</td>\n      <td>2.123633</td>\n      <td>2.067512</td>\n      <td>-0.812675</td>\n      <td>-0.901299</td>\n      <td>0.007745</td>\n      <td>-0.091173</td>\n      <td>0.367606</td>\n      <td>0.485844</td>\n      <td>0.698562</td>\n      <td>0.508801</td>\n      <td>-0.005845</td>\n      <td>-0.159698</td>\n      <td>-1.202045</td>\n      <td>-1.641689</td>\n      <td>-0.613714</td>\n      <td>-0.737990</td>\n      <td>-0.489775</td>\n      <td>-0.277163</td>\n      <td>-0.569776</td>\n      <td>-0.678041</td>\n      <td>-0.737361</td>\n      <td>-0.805210</td>\n      <td>0.642280</td>\n      <td>0.900020</td>\n      <td>1.136675</td>\n      <td>1.392455</td>\n      <td>1.227829</td>\n      <td>1.955003</td>\n      <td>1.272995</td>\n      <td>1.321568</td>\n      <td>1.435162</td>\n      <td>1.310822</td>\n      <td>-0.533306</td>\n      <td>-0.549327</td>\n      <td>3.191047</td>\n      <td>5.121527</td>\n      <td>1.707723</td>\n      <td>0.316878</td>\n      <td>0.367816</td>\n      <td>-0.292742</td>\n      <td>0.048963</td>\n      <td>-0.044358</td>\n      <td>-0.479616</td>\n      <td>-0.478027</td>\n      <td>-0.434722</td>\n      <td>-0.150664</td>\n      <td>1.070104</td>\n      <td>-1.509645</td>\n      <td>-0.231228</td>\n      <td>-0.750288</td>\n      <td>3.563599</td>\n      <td>1.221349</td>\n      <td>3.644364</td>\n      <td>3.476806</td>\n      <td>2.932503</td>\n      <td>6.496593</td>\n      <td>5.987627</td>\n      <td>-0.057488</td>\n      <td>-0.067704</td>\n      <td>3.776708</td>\n      <td>-0.895911</td>\n      <td>-0.644280</td>\n      <td>5.547774</td>\n      <td>5.450953</td>\n      <td>0.668603</td>\n      <td>0.242462</td>\n      <td>0.453440</td>\n      <td>-0.199538</td>\n      <td>1.077485</td>\n      <td>0.644870</td>\n      <td>0.902889</td>\n      <td>1.036488</td>\n      <td>5.731158</td>\n      <td>-0.519771</td>\n      <td>1.260760</td>\n      <td>1.054358</td>\n      <td>1.704637</td>\n      <td>1.202234</td>\n      <td>6.966027</td>\n      <td>0.604938</td>\n      <td>1.304213</td>\n      <td>0.258284</td>\n      <td>0.775861</td>\n      <td>0.983468</td>\n      <td>3.625842</td>\n      <td>1.167519</td>\n      <td>2.451908</td>\n      <td>0.672161</td>\n      <td>1.024876</td>\n      <td>1.163389</td>\n      <td>4.720294</td>\n      <td>0.748591</td>\n      <td>-1.338859</td>\n      <td>-0.612471</td>\n      <td>-0.684489</td>\n      <td>-1.194013</td>\n      <td>2.313679</td>\n      <td>1.470179</td>\n      <td>-0.940190</td>\n      <td>-0.256728</td>\n      <td>-0.762834</td>\n      <td>-1.781693</td>\n      <td>3.344485</td>\n      <td>1.009691</td>\n      <td>-0.099606</td>\n      <td>-0.096172</td>\n      <td>-0.163615</td>\n      <td>-0.449158</td>\n      <td>3.507450</td>\n      <td>1.234475</td>\n      <td>0.655880</td>\n      <td>0.268850</td>\n      <td>0.280304</td>\n      <td>0.185875</td>\n      <td>3.343385</td>\n      <td>-0.518551</td>\n      <td>-1.957893</td>\n      <td>-0.374570</td>\n      <td>-1.104518</td>\n      <td>-0.527078</td>\n      <td>-2.316842</td>\n      <td>-0.775282</td>\n      <td>-1.619046</td>\n      <td>-0.495941</td>\n      <td>-1.155449</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1.0</td>\n      <td>-0.134054</td>\n      <td>1.807014</td>\n      <td>-1.446378</td>\n      <td>-2.836502</td>\n      <td>-0.661081</td>\n      <td>-1.371023</td>\n      <td>-0.147917</td>\n      <td>0.105262</td>\n      <td>-0.486659</td>\n      <td>1.119627</td>\n      <td>-0.197795</td>\n      <td>0.380989</td>\n      <td>-0.506360</td>\n      <td>0.426367</td>\n      <td>-0.276383</td>\n      <td>1.440348</td>\n      <td>0.135558</td>\n      <td>0.600590</td>\n      <td>0.606931</td>\n      <td>2.141194</td>\n      <td>0.337984</td>\n      <td>0.734770</td>\n      <td>0.473697</td>\n      <td>1.834547</td>\n      <td>0.692217</td>\n      <td>2.325833</td>\n      <td>-0.152638</td>\n      <td>-0.591999</td>\n      <td>-0.417052</td>\n      <td>-1.309221</td>\n      <td>-0.121506</td>\n      <td>-0.747417</td>\n      <td>-0.257041</td>\n      <td>-0.770830</td>\n      <td>-0.558100</td>\n      <td>-1.311673</td>\n      <td>0.391446</td>\n      <td>1.058568</td>\n      <td>-1.163661</td>\n      <td>-3.924427</td>\n      <td>-0.521476</td>\n      <td>-1.135330</td>\n      <td>-3.192402</td>\n      <td>-1.475888</td>\n      <td>-0.849966</td>\n      <td>-1.048653</td>\n      <td>-1.920787</td>\n      <td>-1.230339</td>\n      <td>-0.637456</td>\n      <td>-1.377099</td>\n      <td>-1.287673</td>\n      <td>-3.440364</td>\n      <td>-1.359410</td>\n      <td>-0.660428</td>\n      <td>-1.311723</td>\n      <td>-0.564375</td>\n      <td>-1.573127</td>\n      <td>-1.436522</td>\n      <td>-0.330318</td>\n      <td>1.954205</td>\n      <td>1.753940</td>\n      <td>-2.483375</td>\n      <td>-2.858843</td>\n      <td>4.330110</td>\n      <td>3.754628</td>\n      <td>3.223322</td>\n      <td>6.621936</td>\n      <td>6.493943</td>\n      <td>0.568199</td>\n      <td>2.820406</td>\n      <td>2.578847</td>\n      <td>-0.777775</td>\n      <td>-3.037156</td>\n      <td>-1.549058</td>\n      <td>-2.686078</td>\n      <td>-3.047853</td>\n      <td>2.057660</td>\n      <td>-0.705593</td>\n      <td>-1.815569</td>\n      <td>-1.096623</td>\n      <td>-2.313443</td>\n      <td>-1.807671</td>\n      <td>1.321021</td>\n      <td>0.097240</td>\n      <td>-1.238212</td>\n      <td>0.263139</td>\n      <td>1.911020</td>\n      <td>-1.641860</td>\n      <td>1.397935</td>\n      <td>-0.752471</td>\n      <td>-1.515613</td>\n      <td>-0.411844</td>\n      <td>0.518049</td>\n      <td>-1.086886</td>\n      <td>0.238309</td>\n      <td>0.516759</td>\n      <td>4.402721</td>\n      <td>1.861905</td>\n      <td>3.457627</td>\n      <td>3.332977</td>\n      <td>0.939714</td>\n      <td>-0.502837</td>\n      <td>2.570269</td>\n      <td>0.541858</td>\n      <td>2.045397</td>\n      <td>2.700756</td>\n      <td>-0.005350</td>\n      <td>0.447249</td>\n      <td>2.896634</td>\n      <td>0.913320</td>\n      <td>2.405795</td>\n      <td>2.494068</td>\n      <td>1.439167</td>\n      <td>-0.637042</td>\n      <td>1.620776</td>\n      <td>0.228736</td>\n      <td>1.917310</td>\n      <td>1.168638</td>\n      <td>0.081733</td>\n      <td>-1.302271</td>\n      <td>-1.298515</td>\n      <td>-1.496335</td>\n      <td>-1.101116</td>\n      <td>-1.066743</td>\n      <td>-0.949372</td>\n      <td>-1.952977</td>\n      <td>-1.154190</td>\n      <td>-1.757932</td>\n      <td>-1.083780</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>-1.0</td>\n      <td>-0.152553</td>\n      <td>0.019958</td>\n      <td>-0.432943</td>\n      <td>-0.419315</td>\n      <td>-0.885222</td>\n      <td>-0.946822</td>\n      <td>-0.347564</td>\n      <td>-0.415661</td>\n      <td>-2.697451</td>\n      <td>-1.771866</td>\n      <td>0.797991</td>\n      <td>1.597449</td>\n      <td>0.058487</td>\n      <td>0.139840</td>\n      <td>-1.466602</td>\n      <td>-1.948157</td>\n      <td>-0.862309</td>\n      <td>-1.306968</td>\n      <td>-1.360351</td>\n      <td>-2.032700</td>\n      <td>-1.267283</td>\n      <td>-0.792855</td>\n      <td>-1.380421</td>\n      <td>-2.113408</td>\n      <td>-1.705441</td>\n      <td>-2.344440</td>\n      <td>0.656163</td>\n      <td>1.124230</td>\n      <td>0.279730</td>\n      <td>0.383007</td>\n      <td>1.204640</td>\n      <td>2.294535</td>\n      <td>0.816201</td>\n      <td>0.992601</td>\n      <td>0.344200</td>\n      <td>0.358523</td>\n      <td>-0.964182</td>\n      <td>-1.145700</td>\n      <td>0.524740</td>\n      <td>0.944383</td>\n      <td>1.535964</td>\n      <td>-1.116171</td>\n      <td>-0.215730</td>\n      <td>-3.215212</td>\n      <td>-0.613003</td>\n      <td>-0.582541</td>\n      <td>-1.195044</td>\n      <td>-0.984228</td>\n      <td>-0.762597</td>\n      <td>-0.403907</td>\n      <td>0.310702</td>\n      <td>3.034981</td>\n      <td>-1.440876</td>\n      <td>-0.794638</td>\n      <td>-0.244628</td>\n      <td>0.110498</td>\n      <td>-0.039235</td>\n      <td>0.522767</td>\n      <td>0.504679</td>\n      <td>-1.319669</td>\n      <td>-1.105191</td>\n      <td>0.723748</td>\n      <td>0.752707</td>\n      <td>-0.820320</td>\n      <td>0.478335</td>\n      <td>0.394013</td>\n      <td>-0.296511</td>\n      <td>-0.281779</td>\n      <td>-0.766811</td>\n      <td>0.187193</td>\n      <td>-0.496821</td>\n      <td>-0.031945</td>\n      <td>1.210706</td>\n      <td>0.460572</td>\n      <td>0.764506</td>\n      <td>1.189017</td>\n      <td>-0.323691</td>\n      <td>-0.183620</td>\n      <td>1.937619</td>\n      <td>0.996560</td>\n      <td>1.871079</td>\n      <td>1.874827</td>\n      <td>-0.761728</td>\n      <td>-0.848658</td>\n      <td>1.836915</td>\n      <td>-0.180091</td>\n      <td>0.645820</td>\n      <td>1.531352</td>\n      <td>-0.953522</td>\n      <td>-0.392436</td>\n      <td>4.734488</td>\n      <td>0.354859</td>\n      <td>1.421339</td>\n      <td>2.679025</td>\n      <td>-0.189003</td>\n      <td>-0.996342</td>\n      <td>0.261465</td>\n      <td>-1.400234</td>\n      <td>-0.508243</td>\n      <td>-0.083492</td>\n      <td>-0.763670</td>\n      <td>-0.419085</td>\n      <td>0.500733</td>\n      <td>-0.699896</td>\n      <td>-0.301351</td>\n      <td>0.219408</td>\n      <td>-0.065635</td>\n      <td>-1.763346</td>\n      <td>0.773873</td>\n      <td>-0.650471</td>\n      <td>-0.170602</td>\n      <td>0.430337</td>\n      <td>-1.130454</td>\n      <td>-0.420269</td>\n      <td>2.790827</td>\n      <td>-0.235756</td>\n      <td>0.866550</td>\n      <td>2.056568</td>\n      <td>-0.100069</td>\n      <td>-0.869790</td>\n      <td>0.335393</td>\n      <td>-1.214731</td>\n      <td>-0.504311</td>\n      <td>-0.799622</td>\n      <td>0.185756</td>\n      <td>-1.324052</td>\n      <td>-0.052767</td>\n      <td>-1.445105</td>\n      <td>-0.600913</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1.0</td>\n      <td>-0.926307</td>\n      <td>0.014673</td>\n      <td>-1.137026</td>\n      <td>-1.809032</td>\n      <td>-0.195679</td>\n      <td>-0.307055</td>\n      <td>-0.739307</td>\n      <td>-1.176925</td>\n      <td>-0.703951</td>\n      <td>0.367620</td>\n      <td>-0.795503</td>\n      <td>-1.636025</td>\n      <td>-0.608439</td>\n      <td>-0.040905</td>\n      <td>-0.466774</td>\n      <td>0.407433</td>\n      <td>0.089924</td>\n      <td>0.298663</td>\n      <td>0.504540</td>\n      <td>1.387729</td>\n      <td>0.284969</td>\n      <td>0.460544</td>\n      <td>0.406785</td>\n      <td>1.181936</td>\n      <td>0.534446</td>\n      <td>1.405543</td>\n      <td>-0.605565</td>\n      <td>-1.458782</td>\n      <td>-0.454171</td>\n      <td>-1.130895</td>\n      <td>-0.503695</td>\n      <td>-1.553803</td>\n      <td>-0.290390</td>\n      <td>-0.651474</td>\n      <td>-0.618948</td>\n      <td>-1.123614</td>\n      <td>0.018146</td>\n      <td>0.072196</td>\n      <td>-0.959666</td>\n      <td>-2.701256</td>\n      <td>2.745476</td>\n      <td>-1.695491</td>\n      <td>-2.095926</td>\n      <td>-0.200334</td>\n      <td>-0.275669</td>\n      <td>-0.258548</td>\n      <td>-0.524798</td>\n      <td>0.001740</td>\n      <td>0.606657</td>\n      <td>0.610053</td>\n      <td>4.483181</td>\n      <td>0.915275</td>\n      <td>-1.696166</td>\n      <td>-0.296385</td>\n      <td>0.965825</td>\n      <td>0.401357</td>\n      <td>1.633170</td>\n      <td>2.319805</td>\n      <td>0.219550</td>\n      <td>3.087963</td>\n      <td>2.815752</td>\n      <td>-1.914500</td>\n      <td>-2.163979</td>\n      <td>0.664650</td>\n      <td>-1.799856</td>\n      <td>-1.191255</td>\n      <td>2.121957</td>\n      <td>2.088650</td>\n      <td>0.168586</td>\n      <td>3.328743</td>\n      <td>1.339609</td>\n      <td>0.410963</td>\n      <td>-0.933070</td>\n      <td>0.352775</td>\n      <td>-0.473705</td>\n      <td>-0.888991</td>\n      <td>-0.337139</td>\n      <td>0.595054</td>\n      <td>-0.608552</td>\n      <td>0.339639</td>\n      <td>-0.455977</td>\n      <td>-0.568781</td>\n      <td>-0.276601</td>\n      <td>-0.352332</td>\n      <td>-1.238212</td>\n      <td>-0.071089</td>\n      <td>-1.414742</td>\n      <td>-1.641860</td>\n      <td>-0.369484</td>\n      <td>-0.743119</td>\n      <td>-1.515613</td>\n      <td>-0.377835</td>\n      <td>-1.183779</td>\n      <td>-1.086886</td>\n      <td>-0.578727</td>\n      <td>-0.866684</td>\n      <td>1.112136</td>\n      <td>-0.723876</td>\n      <td>-0.069121</td>\n      <td>0.586970</td>\n      <td>-0.291187</td>\n      <td>-1.442057</td>\n      <td>0.614802</td>\n      <td>-0.696013</td>\n      <td>-0.375734</td>\n      <td>0.359990</td>\n      <td>-0.533730</td>\n      <td>-1.073671</td>\n      <td>-0.243579</td>\n      <td>-0.370560</td>\n      <td>-0.622560</td>\n      <td>-0.595979</td>\n      <td>-0.443687</td>\n      <td>-1.043729</td>\n      <td>-0.569033</td>\n      <td>-0.931466</td>\n      <td>-1.700758</td>\n      <td>-0.971717</td>\n      <td>-0.417509</td>\n      <td>-1.460654</td>\n      <td>-0.212700</td>\n      <td>-1.833798</td>\n      <td>-0.728769</td>\n      <td>-1.356237</td>\n      <td>-0.611731</td>\n      <td>-2.488541</td>\n      <td>-0.834888</td>\n      <td>-2.192132</td>\n      <td>-0.787399</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1.0</td>\n      <td>-1.942172</td>\n      <td>-1.277556</td>\n      <td>-1.157202</td>\n      <td>-2.007440</td>\n      <td>-1.104323</td>\n      <td>-2.020301</td>\n      <td>-1.005963</td>\n      <td>-1.823199</td>\n      <td>-0.956238</td>\n      <td>0.255502</td>\n      <td>-1.160616</td>\n      <td>-2.751858</td>\n      <td>-0.617936</td>\n      <td>0.062789</td>\n      <td>-0.623396</td>\n      <td>0.269590</td>\n      <td>0.072232</td>\n      <td>0.297024</td>\n      <td>0.393130</td>\n      <td>1.279839</td>\n      <td>0.235116</td>\n      <td>0.458356</td>\n      <td>0.267409</td>\n      <td>0.957216</td>\n      <td>0.411063</td>\n      <td>1.290201</td>\n      <td>-0.812885</td>\n      <td>-2.131463</td>\n      <td>-0.229063</td>\n      <td>-0.572326</td>\n      <td>-1.042763</td>\n      <td>-2.857318</td>\n      <td>-0.099742</td>\n      <td>-0.276955</td>\n      <td>-0.252227</td>\n      <td>-0.531873</td>\n      <td>0.011499</td>\n      <td>0.071707</td>\n      <td>-0.973653</td>\n      <td>-2.974678</td>\n      <td>0.565780</td>\n      <td>-2.216094</td>\n      <td>-2.546431</td>\n      <td>8.316764</td>\n      <td>8.098654</td>\n      <td>-1.477383</td>\n      <td>-2.835706</td>\n      <td>-2.119287</td>\n      <td>-1.379865</td>\n      <td>-1.770006</td>\n      <td>-1.462099</td>\n      <td>-1.819851</td>\n      <td>-1.108216</td>\n      <td>1.614903</td>\n      <td>-0.268317</td>\n      <td>-0.808520</td>\n      <td>-0.290100</td>\n      <td>-0.103562</td>\n      <td>-2.205421</td>\n      <td>1.353710</td>\n      <td>1.199068</td>\n      <td>-2.233744</td>\n      <td>-2.548284</td>\n      <td>-0.930710</td>\n      <td>-1.686300</td>\n      <td>-1.117824</td>\n      <td>0.383813</td>\n      <td>0.382112</td>\n      <td>-1.346741</td>\n      <td>0.421679</td>\n      <td>-0.322088</td>\n      <td>0.329075</td>\n      <td>0.084546</td>\n      <td>0.081323</td>\n      <td>-0.041499</td>\n      <td>0.078271</td>\n      <td>-0.185436</td>\n      <td>0.867096</td>\n      <td>0.056806</td>\n      <td>0.137553</td>\n      <td>-0.067588</td>\n      <td>0.055031</td>\n      <td>-0.438700</td>\n      <td>-0.241235</td>\n      <td>0.544986</td>\n      <td>-0.397463</td>\n      <td>-0.592483</td>\n      <td>0.201670</td>\n      <td>-0.926826</td>\n      <td>0.180414</td>\n      <td>1.089757</td>\n      <td>-0.201719</td>\n      <td>-0.207605</td>\n      <td>0.346765</td>\n      <td>-0.389871</td>\n      <td>-0.590241</td>\n      <td>0.444328</td>\n      <td>-0.797670</td>\n      <td>-0.301175</td>\n      <td>0.063253</td>\n      <td>-0.770643</td>\n      <td>-0.156742</td>\n      <td>0.591109</td>\n      <td>-0.298508</td>\n      <td>-0.113917</td>\n      <td>0.330774</td>\n      <td>-0.322268</td>\n      <td>-0.746005</td>\n      <td>-0.311291</td>\n      <td>-0.633066</td>\n      <td>-0.580679</td>\n      <td>-0.664988</td>\n      <td>-1.120002</td>\n      <td>0.006400</td>\n      <td>0.643466</td>\n      <td>-0.453981</td>\n      <td>-0.410716</td>\n      <td>0.174669</td>\n      <td>-0.270236</td>\n      <td>-1.256721</td>\n      <td>1.837371</td>\n      <td>-1.247840</td>\n      <td>1.644083</td>\n      <td>-1.026287</td>\n      <td>2.368146</td>\n      <td>-1.618017</td>\n      <td>1.897209</td>\n      <td>-1.490766</td>\n      <td>1.500580</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>-1.0</td>\n      <td>-0.449196</td>\n      <td>0.205344</td>\n      <td>1.426443</td>\n      <td>1.976428</td>\n      <td>0.941308</td>\n      <td>1.337059</td>\n      <td>-0.008373</td>\n      <td>0.127028</td>\n      <td>-0.758190</td>\n      <td>-0.069642</td>\n      <td>-0.219234</td>\n      <td>-0.180030</td>\n      <td>-0.505746</td>\n      <td>-0.156642</td>\n      <td>-0.500818</td>\n      <td>-0.132893</td>\n      <td>-0.197686</td>\n      <td>-0.429510</td>\n      <td>-0.658262</td>\n      <td>-1.231168</td>\n      <td>-0.539522</td>\n      <td>-0.482442</td>\n      <td>-0.621047</td>\n      <td>-1.263574</td>\n      <td>-0.797024</td>\n      <td>-1.410188</td>\n      <td>0.279142</td>\n      <td>0.611139</td>\n      <td>0.592847</td>\n      <td>1.226658</td>\n      <td>0.254766</td>\n      <td>0.567363</td>\n      <td>0.414690</td>\n      <td>0.682051</td>\n      <td>0.743193</td>\n      <td>1.149632</td>\n      <td>-0.096623</td>\n      <td>-0.155102</td>\n      <td>1.609053</td>\n      <td>3.792686</td>\n      <td>-0.559477</td>\n      <td>-0.588074</td>\n      <td>-1.158639</td>\n      <td>-1.210787</td>\n      <td>-0.710006</td>\n      <td>0.697680</td>\n      <td>0.912140</td>\n      <td>1.484675</td>\n      <td>2.351518</td>\n      <td>1.629427</td>\n      <td>0.833016</td>\n      <td>0.039408</td>\n      <td>-1.085349</td>\n      <td>-0.983198</td>\n      <td>-0.616236</td>\n      <td>-0.833880</td>\n      <td>-1.572900</td>\n      <td>-1.875401</td>\n      <td>-2.241706</td>\n      <td>6.925793</td>\n      <td>6.369045</td>\n      <td>-0.115487</td>\n      <td>-0.125922</td>\n      <td>3.912011</td>\n      <td>-2.403588</td>\n      <td>-1.484742</td>\n      <td>5.719479</td>\n      <td>5.616981</td>\n      <td>-0.361736</td>\n      <td>-0.049060</td>\n      <td>0.273322</td>\n      <td>0.956521</td>\n      <td>2.752794</td>\n      <td>0.796185</td>\n      <td>1.702963</td>\n      <td>2.650596</td>\n      <td>-2.150647</td>\n      <td>1.061113</td>\n      <td>1.808534</td>\n      <td>0.623637</td>\n      <td>1.645916</td>\n      <td>1.717268</td>\n      <td>-1.369535</td>\n      <td>0.528552</td>\n      <td>3.994348</td>\n      <td>0.480317</td>\n      <td>1.980458</td>\n      <td>3.483613</td>\n      <td>1.468161</td>\n      <td>-0.144841</td>\n      <td>3.649036</td>\n      <td>-0.042724</td>\n      <td>0.762332</td>\n      <td>1.918046</td>\n      <td>0.497505</td>\n      <td>0.044835</td>\n      <td>-1.338859</td>\n      <td>-0.372465</td>\n      <td>-1.257774</td>\n      <td>-1.194013</td>\n      <td>1.550535</td>\n      <td>-0.829772</td>\n      <td>-0.940190</td>\n      <td>-0.679889</td>\n      <td>-1.510224</td>\n      <td>-1.781693</td>\n      <td>0.679414</td>\n      <td>0.486809</td>\n      <td>2.398606</td>\n      <td>0.122125</td>\n      <td>0.380288</td>\n      <td>1.890917</td>\n      <td>1.863573</td>\n      <td>-0.439641</td>\n      <td>1.536544</td>\n      <td>-0.601939</td>\n      <td>-0.226759</td>\n      <td>0.967284</td>\n      <td>0.444091</td>\n      <td>-1.076396</td>\n      <td>-1.929349</td>\n      <td>-1.225905</td>\n      <td>-1.470428</td>\n      <td>-0.944437</td>\n      <td>-2.007697</td>\n      <td>-1.681480</td>\n      <td>-1.706391</td>\n      <td>-1.467732</td>\n      <td>-1.477835</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"DLs.one_batch ()[2].shape","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"torch.Size([10000, 5])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_cat, x_cont, y = DLs.train.one_batch ()\nx_cat.shape, x_cont.shape, y.shape","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"(torch.Size([10000, 0]), torch.Size([10000, 130]), torch.Size([10000, 5]))"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Custom Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class SmoothBCEwLogits(_WeightedLoss):\n    \n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class FFN (nn.Module):\n    \n    def __init__(self, inputCount=130, outputCount=5, hiddenLayerCounts=[150, 150, 150], \n                 drop_prob=0.2, nonlin=nn.SiLU (), isOpAct=False):\n        \n        super(FFN, self).__init__()\n        \n        self.nonlin     = nonlin\n        self.dropout    = nn.Dropout (drop_prob)\n        self.batchnorm0 = nn.BatchNorm1d (inputCount)\n        self.dense1     = nn.Linear (inputCount, hiddenLayerCounts[0])\n        self.batchnorm1 = nn.BatchNorm1d (hiddenLayerCounts[0])\n        self.dense2     = nn.Linear(hiddenLayerCounts[0], hiddenLayerCounts[1])\n        self.batchnorm2 = nn.BatchNorm1d (hiddenLayerCounts[1])\n        self.dense3     = nn.Linear(hiddenLayerCounts[1], hiddenLayerCounts[2])\n        self.batchnorm3 = nn.BatchNorm1d (hiddenLayerCounts[2])        \n        self.outDense   = None\n        if outputCount > 0:\n            self.outDense   = nn.Linear (hiddenLayerCounts[-1], outputCount)\n        self.outActivtn = None\n        if isOpAct:\n            if outputCount == 1 or outputCount == 2:\n                self.outActivtn = nn.Sigmoid ()\n            elif outputCount > 0:\n                self.outActivtn = nn.Softmax (dim=-1)\n        return\n\n    def forward (self, X):\n        \n        X = self.dropout (self.batchnorm0 (X))\n        X = self.dropout (self.nonlin (self.batchnorm1 (self.dense1 (X))))\n        X = self.dropout (self.nonlin (self.batchnorm2 (self.dense2 (X))))\n        X = self.dropout (self.nonlin (self.batchnorm3 (self.dense3 (X))))\n        if self.outDense:\n            X = self.outDense (X)\n        if self.outActivtn:\n            X = self.outActivtn (X)\n        return X","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Emb_NN_Model (nn.Module):\n    \n    def __init__(self, three_hidden_layers=THREE_HIDDEN_LAYERS, embed_dim=(N_FEAT_TAGS//2+1), csv_file='../input/jane-street-market-prediction/features.csv'):\n        \n        super (Emb_NN_Model, self).__init__()\n        global N_FEAT_TAGS\n        N_FEAT_TAGS = 29\n        \n        # store the features to tags mapping as a datframe tdf, feature_i mapping is in tdf[i, :]\n        dtype = {'tag_0' : 'int8'}\n        for i in range (1, 29):\n            k = 'tag_' + str (i)\n            dtype[k] = 'int8'\n        t_df = pd.read_csv (csv_file, usecols=range (1,N_FEAT_TAGS+1), dtype=dtype)\n        t_df['tag_29'] = np.array ([1] + ([0] * (t_df.shape[0]-1)) ).astype ('int8')\n        self.features_tag_matrix = torch.tensor (t_df.to_numpy ())\n        N_FEAT_TAGS += 1\n        \n        # print ('self.features_tag_matrix =', self.features_tag_matrix)\n        \n        # embeddings for the tags. Each feature is taken a an embedding which is an avg. of its' tag embeddings\n        self.embed_dim     = embed_dim\n        self.tag_embedding = nn.Embedding (N_FEAT_TAGS+1, embed_dim) # create a special tag if not known tag for any feature\n        self.tag_weights   = nn.Linear (N_FEAT_TAGS, 1)\n        \n        self.dropout       = nn.Dropout (0.2)\n        # self.layer_normal= nn.LayerNorm (embed_dim) \n        self.ffn           = FFN (inputCount=(130+embed_dim), outputCount=0, hiddenLayerCounts=[(three_hidden_layers[0]+embed_dim), (three_hidden_layers[1]+embed_dim), (three_hidden_layers[2]+embed_dim)], drop_prob=0.2)\n        self.outDense      = nn.Linear (three_hidden_layers[2]+embed_dim, 5)\n        # self.outActivtn  = nn.LogSoftmax (dim=1)                  # not used\n        self.criterion     = nn.BCEWithLogitsLoss ()    # nn.NLLLoss ()\n        return\n    \n    def features2emb (self):\n        \"\"\"\n        idx : int feature index 0 to N_FEATURES-1 (129)\n        \"\"\"\n        \n        all_tag_idxs = torch.LongTensor (np.arange (N_FEAT_TAGS)) #.to (DEVICE)              # (29,)\n        tag_bools    = self.features_tag_matrix                                # (130, 29)\n        # print ('tag_bools.shape =', tag_bools.size())\n        f_emb        = self.tag_embedding (all_tag_idxs).repeat (130, 1, 1)    #;print ('1. f_emb =', f_emb) # (29, 7) * (130, 1, 1) = (130, 29, 7)\n        # print ('f_emb.shape =', f_emb.size())\n        f_emb        = f_emb * tag_bools[:, :, None]                           #;print ('2. f_emb =', f_emb) # (130, 29, 7) * (130, 29, 1) = (130, 29, 7)\n        # print ('f_emb.shape =', f_emb.size())\n        \n        # Take avg. of all the present tag's embeddings to get the embedding for a feature\n        s = torch.sum (tag_bools, dim=1)                                       # (130,)\n        # print ('s =', s)              \n        f_emb = torch.sum (f_emb, dim=-2) / s[:, None]                         # (130, 7)\n        # print ('f_emb =', f_emb)        \n        # print ('f_emb.shape =', f_emb.shape)\n        \n        # take a linear combination of the present tag's embeddings\n        # f_emb = f_emb.permute (0, 2, 1)                                        # (130, 7, 29)\n        # f_emb = self.tag_weights (f_emb)                      #;print ('3. f_emb =', f_emb)                 # (130, 7, 1)\n        # f_emb = torch.squeeze (f_emb, dim=-1)                 #;print ('4. f_emb =', f_emb)                 # (130, 7)\n        return f_emb\n    \n    def forward (self, cat_featrs, features):\n        \"\"\"\n        when you call `model (x ,y, z, ...)` then this method is invoked\n        \"\"\"\n        \n        cat_featrs = None\n        features   = features.view (-1, N_FEATURES)\n        f_emb      = self.features2emb ()                                #;print ('5. f_emb =', f_emb); print ('6. features =', features) # (130, 7)\n        # print ('features.shape =', features.shape, 'f_emb.shape =', f_emb.shape)\n        features_2 = torch.matmul (features, f_emb)                      #;print ('7. features =', features) # (1, 130) * (130, 7) = (1, 7)\n        # print ('features.shape =', features.shape)\n        \n        # Concatenate the two features (features + their embeddings)\n        features   = torch.hstack ((features, features_2))        \n        \n        x          = self.ffn (features)                               #;print ('8. x.shape = ', x.shape, 'x =', x)   # (1, 7) -> (1, 7)\n        # x        = self.layer_normal (x + features)                  #;print ('9. x.shape = ', x.shape, 'x =', x)   # (1, 7) -> (1, 2)\n        out_logits = self.outDense (x)                                 #;print ('10. out_\n        out_logits = self.outDense (x)                                 #;print ('10. out_logits.shape = ', out_logits.shape, 'out_logits =', out_logits)        \n        # return sigmoid probs\n        # out_probs = F.sigmoid (out_logits)\n        return out_logits","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for vanilla NN use this\n# path  = \"../input/jane-smooth-mynn5-auc-150x3/Jane_mynn5_auc_150_150_150\"\nlearn = TabularLearner (DLs, model=Emb_NN_Model (), \n                        loss_func=SmoothBCEwLogits (smoothing=0.009), metrics=RocAucMulti ())  #, model_dir='/kaggle/working/')\n# learn = learn.load (path)\nlearn.summary ()","execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"Emb_NN_Model (Input shape: 10000 x torch.Size([10000, 130]))\n============================================================================\nLayer (type)         Output Shape         Param #    Trainable \n============================================================================\n                     10000 x 16          \nEmbedding                                 496        True      \nSiLU                                                           \nDropout                                                        \nBatchNorm1d                               292        True      \n____________________________________________________________________________\n                     10000 x 166         \nLinear                                    24402      True      \nBatchNorm1d                               332        True      \nLinear                                    27722      True      \nBatchNorm1d                               332        True      \nLinear                                    27722      True      \nBatchNorm1d                               332        True      \n____________________________________________________________________________\n                     10000 x 5           \nLinear                                    835        True      \n____________________________________________________________________________\n\nTotal params: 82,465\nTotal trainable params: 82,465\nTotal non-trainable params: 0\n\nOptimizer used: <function Adam at 0x7f1c8723bc20>\nLoss function: SmoothBCEwLogits()\n\nCallbacks:\n  - TrainEvalCallback\n  - Recorder\n  - ProgressCallback"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"logits = learn.model (x_cat, x_cont)\nlogits","execution_count":20,"outputs":[{"output_type":"execute_result","execution_count":20,"data":{"text/plain":"tensor([[ 0.1925, -0.0872,  0.0717,  0.0067,  0.1129],\n        [ 0.3268, -0.3634, -0.5046,  1.0127, -0.2045],\n        [ 0.0207, -0.1604,  0.0727,  0.0535, -0.0425],\n        ...,\n        [ 0.2621, -0.0573,  0.0957,  0.0112, -0.0294],\n        [ 0.1559, -0.2738, -0.1338,  0.0238,  0.0361],\n        [ 0.2821,  0.1339,  0.0069, -0.0197, -0.0609]],\n       grad_fn=<AddmmBackward>)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_cat, x_cont, y = learn.dls.one_batch ()\ninit_loss = learn.loss_func (learn.model (x_cat, x_cont), y)\ninit_loss","execution_count":21,"outputs":[{"output_type":"execute_result","execution_count":21,"data":{"text/plain":"tensor(0.7112, grad_fn=<MeanBackward0>)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_min, lr_steep = learn.lr_find (start_lr=1e-4, end_lr=0.1, num_it=100)\nlr_min, lr_steep","execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"execute_result","execution_count":22,"data":{"text/plain":"(0.00660693421959877, 0.0001995262282434851)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelfile='Jane_EmbNN5_auc_'+str (THREE_HIDDEN_LAYERS).replace (' ', '_').replace (',', '').replace ('[', '').replace (']', '')\ncallbacks = [\n    EarlyStoppingCallback (monitor='roc_auc_score', min_delta=0.00001, patience=20),\n    SaveModelCallback (monitor='roc_auc_score', fname=modelfile),\n    ReduceLROnPlateau (monitor='roc_auc_score', min_delta=0.0001, factor=2.0, min_lr=1e-8, patience=1),\n    GradientClip (1.0)\n]\n\nepochs  = 200\nlr      = lr_min\nlearn.fit_one_cycle (epochs, lr, wd=1e-2, cbs=callbacks)","execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>roc_auc_score</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.690101</td>\n      <td>0.688798</td>\n      <td>0.547112</td>\n      <td>01:42</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}},{"output_type":"stream","text":"Better model found at epoch 0 with roc_auc_score value: 0.5471118468464006.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai.imports import *\nfrom fastai.torch_core import *\nfrom fastai.learner import *\n    \n@patch\n@delegates(subplots)\ndef plot_metrics(self: Recorder, nrows=None, ncols=None, figsize=None, **kwargs):\n    metrics = np.stack(self.values)\n    names = self.metric_names[1:-1]\n    n = len(names) - 1\n    if nrows is None and ncols is None:\n        nrows = int(math.sqrt(n))\n        ncols = int(np.ceil(n / nrows))\n    elif nrows is None: nrows = int(np.ceil(n / ncols))\n    elif ncols is None: ncols = int(np.ceil(n / nrows))\n    figsize = figsize or (ncols * 6, nrows * 4)\n    fig, axs = subplots(nrows, ncols, figsize=figsize, **kwargs)\n    axs = [ax if i < n else ax.set_axis_off() for i, ax in enumerate(axs.flatten())][:n]\n    for i, (name, ax) in enumerate(zip(names, [axs[0]] + axs)):\n        ax.plot(metrics[:, i], color='#1f77b4' if i == 0 else '#ff7f0e', label='valid' if i > 0 else 'train')\n        ax.set_title(name if i > 1 else 'losses')\n        ax.legend(loc='best')\n    plt.show()","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.recorder.plot_loss (skip_start=0, with_valid=True)","execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.recorder.plot_metrics ()","execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 864x288 with 2 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"lr_min, lr_steep = learn.lr_find (start_lr=1e-4, end_lr=0.1, num_it=100)\nlr_min, lr_steep"},{"metadata":{"trusted":false},"cell_type":"markdown","source":"epochs  = 80\nlr      = lr_min\nlearn.fit_one_cycle (epochs, lr, wd=1e-2, cbs=callbacks)"},{"metadata":{"trusted":false},"cell_type":"markdown","source":"lr_min, lr_steep = learn.lr_find (start_lr=1e-4, end_lr=0.1, num_it=100)\nlr_min, lr_steep"},{"metadata":{"trusted":false},"cell_type":"markdown","source":"epochs  = 30\nlr      = lr_min\nlearn.fit_one_cycle (epochs, lr, wd=1e-2, cbs=callbacks)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# learn.recorder.plot_lr ()","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# _, logits, _ = learn.predict (DF.iloc[0])\n# logits","execution_count":28,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"MODEL = learn.model.eval ()\nMODEL","execution_count":29,"outputs":[{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"Emb_NN_Model(\n  (tag_embedding): Embedding(31, 16)\n  (tag_weights): Linear(in_features=30, out_features=1, bias=True)\n  (dropout): Dropout(p=0.0, inplace=False)\n  (ffn): FFN(\n    (nonlin): SiLU()\n    (dropout): Dropout(p=0.2, inplace=False)\n    (batchnorm0): BatchNorm1d(146, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (dense1): Linear(in_features=146, out_features=166, bias=True)\n    (batchnorm1): BatchNorm1d(166, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (dense2): Linear(in_features=166, out_features=166, bias=True)\n    (batchnorm2): BatchNorm1d(166, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (dense3): Linear(in_features=166, out_features=166, bias=True)\n    (batchnorm3): BatchNorm1d(166, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (outDense): Linear(in_features=166, out_features=5, bias=True)\n  (criterion): BCEWithLogitsLoss()\n)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.utils.rnn as rnn_utils\nfrom   torch.autograd import Variable\nfrom   torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n\nDEVICE = torch.device (\"cuda:0\") if torch.cuda.is_available () else torch.device (\"cpu\")","execution_count":30,"outputs":[]},{"metadata":{"papermill":{"duration":0.026076,"end_time":"2021-01-22T17:58:55.522497","exception":false,"start_time":"2021-01-22T17:58:55.496421","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# For direct submission, without using Fastai since its too slow\nUse Fastai for training models only, not for prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_torch (test_df):\n    \n    test_df.drop (columns=['weight', 'date'], inplace=True)\n    test_df.reset_index (drop=True, inplace=True)\n    test_df = PIPE.transform (test_df)        \n    test_df = torch.tensor (test_df).float ().view (-1, 130)\n    predictions = []\n    for i in range (test_df.shape[0]):\n        \n        pred_p = torch.sigmoid (MODEL (None, test_df)).detach ().cpu ().numpy ().reshape ((-1, 5))\n        predictions.append (pred_p)\n\n    predictions = np.vstack (predictions)                     #;print ('predictions.shape =', predictions.shape)\n    predictions = np.median (predictions, axis=1)\n    return (predictions >= 0.5).astype (int)","execution_count":31,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# For prediction using Fastai\nDon't use thism its too slow and times out. Use Pytorch for prediction.\nUse Fastai for training the Pytorch models only."},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn.functional as F\n\ndef predict (df, threshold=0.50):\n    \n    dl     = learn.dls.test_dl (df)\n    logits = learn.get_preds (dl=dl)[0]\n    probs  = F.sigmoid (logits).detach ().numpy ()\n    pred   = (np.median (probs, axis=1) >= threshold).astype (int)\n    return pred","execution_count":32,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"test_df = DF.copy()\nresp_cols  = ['resp_1', 'resp_2', 'resp_3','resp_4', 'resp']\ntest_df.drop (columns=resp_cols, inplace=True)\n\ntest_df  = preprocess_data (df=test_df, isTrainData=False)\npredict (test_df)"},{"metadata":{"papermill":{"duration":0.02371,"end_time":"2021-01-22T17:58:55.388369","exception":false,"start_time":"2021-01-22T17:58:55.364659","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Test"},{"metadata":{"execution":{"iopub.execute_input":"2021-01-22T17:58:55.446325Z","iopub.status.busy":"2021-01-22T17:58:55.445226Z","iopub.status.idle":"2021-01-22T17:58:55.471692Z","shell.execute_reply":"2021-01-22T17:58:55.472274Z"},"papermill":{"duration":0.058761,"end_time":"2021-01-22T17:58:55.472431","exception":false,"start_time":"2021-01-22T17:58:55.41367","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import janestreet\nenv      = janestreet.make_env ()  # initialize the environment\nenv_iter = env.iter_test ()        # an iterator which loops over the test set","execution_count":33,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-22T17:58:55.57927Z","iopub.status.busy":"2021-01-22T17:58:55.577702Z","iopub.status.idle":"2021-01-22T18:03:14.623339Z","shell.execute_reply":"2021-01-22T18:03:14.62262Z"},"papermill":{"duration":259.074066,"end_time":"2021-01-22T18:03:14.623458","exception":false,"start_time":"2021-01-22T17:58:55.549392","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"for test_df, pred_df in env_iter:\n    if test_df[\"weight\"].item () > 0:\n        \n        pred_df.action = predict_torch (test_df)\n    else:\n        pred_df.action = 0\n        \n    # print (pred_df)\n    # print (\"--------------\")\n    env.predict (pred_df)","execution_count":34,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-22T18:03:14.677426Z","iopub.status.busy":"2021-01-22T18:03:14.676598Z","iopub.status.idle":"2021-01-22T18:03:14.679698Z","shell.execute_reply":"2021-01-22T18:03:14.680217Z"},"papermill":{"duration":0.032627,"end_time":"2021-01-22T18:03:14.680369","exception":false,"start_time":"2021-01-22T18:03:14.647742","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"print ('Done !')","execution_count":35,"outputs":[{"output_type":"stream","text":"Done !\n","name":"stdout"}]},{"metadata":{"papermill":{"duration":0.026572,"end_time":"2021-01-22T18:03:14.733653","exception":false,"start_time":"2021-01-22T18:03:14.707081","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}