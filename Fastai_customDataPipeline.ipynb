{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-22T17:55:45.967945Z",
     "iopub.status.busy": "2021-01-22T17:55:45.967126Z",
     "iopub.status.idle": "2021-01-22T17:55:46.016606Z",
     "shell.execute_reply": "2021-01-22T17:55:46.015938Z"
    },
    "papermill": {
     "duration": 0.080791,
     "end_time": "2021-01-22T17:55:46.016735",
     "exception": false,
     "start_time": "2021-01-22T17:55:45.935944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-22T17:55:46.078797Z",
     "iopub.status.busy": "2021-01-22T17:55:46.077856Z",
     "iopub.status.idle": "2021-01-22T17:55:58.435764Z",
     "shell.execute_reply": "2021-01-22T17:55:58.434712Z"
    },
    "papermill": {
     "duration": 12.397325,
     "end_time": "2021-01-22T17:55:58.435940",
     "exception": false,
     "start_time": "2021-01-22T17:55:46.038615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gc\n",
    "import random\n",
    "from   tqdm import tqdm\n",
    "from   sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import psutil\n",
    "import datatable as dt\n",
    "from   collections import namedtuple\n",
    "import os\n",
    "import seaborn as sns\n",
    "from   sklearn.utils import shuffle\n",
    "import datetime\n",
    "import time\n",
    "from   copy import deepcopy as copy\n",
    "from   sklearn.metrics import accuracy_score, precision_recall_fscore_support, matthews_corrcoef, roc_auc_score\n",
    "from   sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score, r2_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from   torch.utils.data import RandomSampler, SequentialSampler  #, Dataset, DataLoader\n",
    "from   transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "import fastai\n",
    "from   fastai.callback import *\n",
    "from   fastai.callback.all import *\n",
    "from   fastai.callback.training import GradientClip\n",
    "from   fastai.callback.all import SaveModelCallback, EarlyStoppingCallback, ReduceLROnPlateau \n",
    "from   fastai.tabular import *\n",
    "from   fastai.tabular.data import *\n",
    "from   fastai.tabular.all import *\n",
    "from   fastai.tabular.all import TabularPandas, RandomSplitter, CategoryBlock, MultiCategoryBlock, range_of, accuracy, tabular_learner, TabularDataLoaders\n",
    "# from   fastai import datasets\n",
    "# from   fastai.dataset import ModelData,ArraysIndexDataset\n",
    "# from   fastai.dataloader import DataLoader\n",
    "from   fastai.learner import Learner\n",
    "\n",
    "from   functools import partial\n",
    "\n",
    "from   sklearn.pipeline import Pipeline\n",
    "from   sklearn.impute import SimpleImputer\n",
    "from   sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings (\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(fastai.data.core.Datasets,\n",
       " fastai.data.load.DataLoader,\n",
       " fastai.data.core.DataLoaders)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Datasets, DataLoader, DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Vars\n",
    "BS     = 10000\n",
    "TP     = None\n",
    "DF     = None\n",
    "DLs    = None\n",
    "PIPE   = None\n",
    "F_COLS = None\n",
    "R_COLS = None\n",
    "EMB5_MODEL = None\n",
    "N_FEATURES = 0\n",
    "TRAIN_DL   = None # train DataLoader \n",
    "VAL_DL     = None   # valid DataLoader\n",
    "HIST_LEN   = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130, 29)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtype = {\n",
    "    'feature'  : 'str', \n",
    "    'tag_0'    : 'int8'\n",
    "}\n",
    "for i in range (1, 29):\n",
    "    k = 'tag_' + str (i)\n",
    "    dtype[k] = 'int8'\n",
    "    \n",
    "features_df = pd.read_csv ('../input/jane-street-market-prediction/features.csv', usecols=range (1,30), dtype=dtype)\n",
    "N_FEATURES  = features_df.shape[0]  # the features.csv has 130 features (1st row) = no of features in train.csv (feature_0 to feature_129)\n",
    "N_FEAT_TAGS = features_df.shape[1]  # the features.csv has 29 tags\n",
    "# features_df.head ()\n",
    "del features_df\n",
    "gc.collect ()\n",
    "N_FEATURES, N_FEAT_TAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-22T17:55:58.538074Z",
     "iopub.status.busy": "2021-01-22T17:55:58.537413Z",
     "iopub.status.idle": "2021-01-22T17:55:58.552729Z",
     "shell.execute_reply": "2021-01-22T17:55:58.553283Z"
    },
    "papermill": {
     "duration": 0.040645,
     "end_time": "2021-01-22T17:55:58.553426",
     "exception": false,
     "start_time": "2021-01-22T17:55:58.512781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_data (filename='../input/jane-street-market-prediction/train.csv.dummy', df=None, isTrainData=True):\n",
    "    \n",
    "    global PIPE, F_COLS, R_COLS\n",
    "    dtype = None\n",
    "    if isTrainData:\n",
    "        \n",
    "        dtype = {\n",
    "            'date'      : 'int64', \n",
    "            'weight'    : 'float32',\n",
    "            'resp'      : 'float32',\n",
    "            'ts_id'     : 'int64',  \n",
    "            'feature_0' : 'float32'\n",
    "        }\n",
    "    else:\n",
    "        \n",
    "        dtype = {\n",
    "            'date'      : 'int64', \n",
    "            'weight'    : 'float32',\n",
    "            'feature_0' : 'float32'\n",
    "        }\n",
    "    for i in range (1, 130):\n",
    "        k = 'feature_' + str (i)\n",
    "        dtype[k] = 'float32'\n",
    "    \n",
    "    to = None\n",
    "    df = None\n",
    "    if isTrainData:\n",
    "        df         = pd.read_csv (filename, dtype=dtype)\n",
    "        df         = df.query ('date > 85')\n",
    "        df         = df[df['weight'] != 0].reset_index (drop = True)\n",
    "        \n",
    "        R_COLS     = ['resp_1', 'resp_2', 'resp_3','resp_4', 'resp']    \n",
    "        # df[:5000].to_csv (filename+'.dummy', index=False) \n",
    "        y          = np.stack ([(df[c] > 0).astype ('int') for c in R_COLS]).T\n",
    "        df.drop (columns=['weight', 'date', 'ts_id']+R_COLS, inplace=True)\n",
    "        F_COLS     = [c for c in df.columns if \"feature\" in c]\n",
    "        PIPE       = Pipeline ([\n",
    "                        (\"imputer\", SimpleImputer (missing_values=np.nan, strategy='mean')),\n",
    "                        # (\"stand\",   StandardScaler (with_mean=False))\n",
    "        ])\n",
    "        columns    = list (df.columns) + R_COLS\n",
    "        X          = PIPE.fit_transform (df)\n",
    "        df         = pd.DataFrame (np.hstack ((X, y)))\n",
    "        df.columns = columns\n",
    "        del X, y\n",
    "        \n",
    "        splits     = RandomSplitter (valid_pct=0.05) (range_of (df))\n",
    "        to         = TabularPandas (df, cont_names=F_COLS, cat_names=None, y_names=R_COLS, y_block=MultiCategoryBlock(encoded=True, vocab=R_COLS), splits=splits)        \n",
    "        return to, df\n",
    "    else:\n",
    "        \n",
    "        df         = df.drop (columns=['weight', 'date']).reset_index (drop = True)\n",
    "        columns    = df.columns\n",
    "        X          = PIPE.transform (df)\n",
    "        df         = pd.DataFrame (X)\n",
    "        df.columns = columns\n",
    "        # del X            \n",
    "        return to, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-22T17:55:58.601049Z",
     "iopub.status.busy": "2021-01-22T17:55:58.600425Z",
     "iopub.status.idle": "2021-01-22T17:58:44.347053Z",
     "shell.execute_reply": "2021-01-22T17:58:44.347603Z"
    },
    "papermill": {
     "duration": 165.771921,
     "end_time": "2021-01-22T17:58:44.347759",
     "exception": false,
     "start_time": "2021-01-22T17:55:58.575838",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_120</th>\n",
       "      <th>feature_121</th>\n",
       "      <th>feature_122</th>\n",
       "      <th>feature_123</th>\n",
       "      <th>feature_124</th>\n",
       "      <th>feature_125</th>\n",
       "      <th>feature_126</th>\n",
       "      <th>feature_127</th>\n",
       "      <th>feature_128</th>\n",
       "      <th>feature_129</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4199</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.079090</td>\n",
       "      <td>2.116994</td>\n",
       "      <td>0.363790</td>\n",
       "      <td>0.810077</td>\n",
       "      <td>-0.319050</td>\n",
       "      <td>-0.638149</td>\n",
       "      <td>1.119922</td>\n",
       "      <td>3.032730</td>\n",
       "      <td>-0.890097</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.257776</td>\n",
       "      <td>2.195189</td>\n",
       "      <td>-1.357857</td>\n",
       "      <td>-1.184972</td>\n",
       "      <td>-0.237498</td>\n",
       "      <td>2.246579</td>\n",
       "      <td>-0.365514</td>\n",
       "      <td>1.652337</td>\n",
       "      <td>-1.203238</td>\n",
       "      <td>-0.397621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4285</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.749787</td>\n",
       "      <td>-0.696183</td>\n",
       "      <td>-2.939075</td>\n",
       "      <td>-3.331175</td>\n",
       "      <td>-6.672603</td>\n",
       "      <td>-7.583367</td>\n",
       "      <td>-1.989951</td>\n",
       "      <td>-2.625277</td>\n",
       "      <td>1.304397</td>\n",
       "      <td>...</td>\n",
       "      <td>1.036693</td>\n",
       "      <td>0.240109</td>\n",
       "      <td>0.631677</td>\n",
       "      <td>-0.735273</td>\n",
       "      <td>0.683545</td>\n",
       "      <td>-0.499444</td>\n",
       "      <td>0.942777</td>\n",
       "      <td>-0.498379</td>\n",
       "      <td>0.744161</td>\n",
       "      <td>-0.593055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "4199        1.0   0.079090   2.116994   0.363790   0.810077  -0.319050   \n",
       "4285       -1.0  -0.749787  -0.696183  -2.939075  -3.331175  -6.672603   \n",
       "\n",
       "      feature_6  feature_7  feature_8  feature_9  ...  feature_120  \\\n",
       "4199  -0.638149   1.119922   3.032730  -0.890097  ...    -0.257776   \n",
       "4285  -7.583367  -1.989951  -2.625277   1.304397  ...     1.036693   \n",
       "\n",
       "      feature_121  feature_122  feature_123  feature_124  feature_125  \\\n",
       "4199     2.195189    -1.357857    -1.184972    -0.237498     2.246579   \n",
       "4285     0.240109     0.631677    -0.735273     0.683545    -0.499444   \n",
       "\n",
       "      feature_126  feature_127  feature_128  feature_129  \n",
       "4199    -0.365514     1.652337    -1.203238    -0.397621  \n",
       "4285     0.942777    -0.498379     0.744161    -0.593055  \n",
       "\n",
       "[2 rows x 130 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TP, DF = preprocess_data ()\n",
    "TP.xs.iloc[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_125</th>\n",
       "      <th>feature_126</th>\n",
       "      <th>feature_127</th>\n",
       "      <th>feature_128</th>\n",
       "      <th>feature_129</th>\n",
       "      <th>resp_1</th>\n",
       "      <th>resp_2</th>\n",
       "      <th>resp_3</th>\n",
       "      <th>resp_4</th>\n",
       "      <th>resp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.151305</td>\n",
       "      <td>5.467693</td>\n",
       "      <td>-0.164505</td>\n",
       "      <td>-0.189219</td>\n",
       "      <td>0.663966</td>\n",
       "      <td>0.988896</td>\n",
       "      <td>0.041933</td>\n",
       "      <td>0.023107</td>\n",
       "      <td>2.184804</td>\n",
       "      <td>...</td>\n",
       "      <td>4.331030</td>\n",
       "      <td>2.553220</td>\n",
       "      <td>3.799011</td>\n",
       "      <td>2.642943</td>\n",
       "      <td>3.998054</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.365888</td>\n",
       "      <td>0.824004</td>\n",
       "      <td>-0.293208</td>\n",
       "      <td>-0.416391</td>\n",
       "      <td>-0.599185</td>\n",
       "      <td>-0.997330</td>\n",
       "      <td>0.041933</td>\n",
       "      <td>0.023107</td>\n",
       "      <td>-0.869330</td>\n",
       "      <td>...</td>\n",
       "      <td>4.133183</td>\n",
       "      <td>-1.207878</td>\n",
       "      <td>3.402796</td>\n",
       "      <td>-0.928290</td>\n",
       "      <td>3.511141</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.514607</td>\n",
       "      <td>0.596214</td>\n",
       "      <td>0.324062</td>\n",
       "      <td>0.154730</td>\n",
       "      <td>0.845069</td>\n",
       "      <td>0.521491</td>\n",
       "      <td>0.041933</td>\n",
       "      <td>0.023107</td>\n",
       "      <td>0.310387</td>\n",
       "      <td>...</td>\n",
       "      <td>4.613493</td>\n",
       "      <td>4.516109</td>\n",
       "      <td>3.341374</td>\n",
       "      <td>2.635798</td>\n",
       "      <td>1.535235</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.008999</td>\n",
       "      <td>0.242132</td>\n",
       "      <td>-0.076187</td>\n",
       "      <td>-0.073584</td>\n",
       "      <td>-0.330932</td>\n",
       "      <td>-0.634109</td>\n",
       "      <td>0.041933</td>\n",
       "      <td>0.023107</td>\n",
       "      <td>-2.315807</td>\n",
       "      <td>...</td>\n",
       "      <td>1.645537</td>\n",
       "      <td>-1.318172</td>\n",
       "      <td>1.369452</td>\n",
       "      <td>-1.352576</td>\n",
       "      <td>0.733124</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.391784</td>\n",
       "      <td>0.152261</td>\n",
       "      <td>-0.603803</td>\n",
       "      <td>-0.711527</td>\n",
       "      <td>-1.204166</td>\n",
       "      <td>-1.579647</td>\n",
       "      <td>0.041933</td>\n",
       "      <td>0.023107</td>\n",
       "      <td>-1.926413</td>\n",
       "      <td>...</td>\n",
       "      <td>2.949878</td>\n",
       "      <td>-2.033542</td>\n",
       "      <td>2.248799</td>\n",
       "      <td>-1.838696</td>\n",
       "      <td>1.905638</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 135 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "0        1.0   3.151305   5.467693  -0.164505  -0.189219   0.663966   \n",
       "1       -1.0  -0.365888   0.824004  -0.293208  -0.416391  -0.599185   \n",
       "2        1.0   1.514607   0.596214   0.324062   0.154730   0.845069   \n",
       "3       -1.0  -1.008999   0.242132  -0.076187  -0.073584  -0.330932   \n",
       "4       -1.0  -0.391784   0.152261  -0.603803  -0.711527  -1.204166   \n",
       "\n",
       "   feature_6  feature_7  feature_8  feature_9  ...  feature_125  feature_126  \\\n",
       "0   0.988896   0.041933   0.023107   2.184804  ...     4.331030     2.553220   \n",
       "1  -0.997330   0.041933   0.023107  -0.869330  ...     4.133183    -1.207878   \n",
       "2   0.521491   0.041933   0.023107   0.310387  ...     4.613493     4.516109   \n",
       "3  -0.634109   0.041933   0.023107  -2.315807  ...     1.645537    -1.318172   \n",
       "4  -1.579647   0.041933   0.023107  -1.926413  ...     2.949878    -2.033542   \n",
       "\n",
       "   feature_127  feature_128  feature_129  resp_1  resp_2  resp_3  resp_4  resp  \n",
       "0     3.799011     2.642943     3.998054     0.0     0.0     0.0     0.0   0.0  \n",
       "1     3.402796    -0.928290     3.511141     1.0     0.0     0.0     0.0   0.0  \n",
       "2     3.341374     2.635798     1.535235     1.0     1.0     0.0     0.0   1.0  \n",
       "3     1.369452    -1.352576     0.733124     1.0     1.0     0.0     0.0   0.0  \n",
       "4     2.248799    -1.838696     1.905638     1.0     1.0     1.0     1.0   1.0  \n",
       "\n",
       "[5 rows x 135 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF.head ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>feature_11</th>\n",
       "      <th>feature_12</th>\n",
       "      <th>feature_13</th>\n",
       "      <th>feature_14</th>\n",
       "      <th>feature_15</th>\n",
       "      <th>feature_16</th>\n",
       "      <th>feature_17</th>\n",
       "      <th>feature_18</th>\n",
       "      <th>feature_19</th>\n",
       "      <th>feature_20</th>\n",
       "      <th>feature_21</th>\n",
       "      <th>feature_22</th>\n",
       "      <th>feature_23</th>\n",
       "      <th>feature_24</th>\n",
       "      <th>feature_25</th>\n",
       "      <th>feature_26</th>\n",
       "      <th>feature_27</th>\n",
       "      <th>feature_28</th>\n",
       "      <th>feature_29</th>\n",
       "      <th>feature_30</th>\n",
       "      <th>feature_31</th>\n",
       "      <th>feature_32</th>\n",
       "      <th>feature_33</th>\n",
       "      <th>feature_34</th>\n",
       "      <th>feature_35</th>\n",
       "      <th>feature_36</th>\n",
       "      <th>feature_37</th>\n",
       "      <th>feature_38</th>\n",
       "      <th>feature_39</th>\n",
       "      <th>feature_40</th>\n",
       "      <th>feature_41</th>\n",
       "      <th>feature_42</th>\n",
       "      <th>feature_43</th>\n",
       "      <th>feature_44</th>\n",
       "      <th>feature_45</th>\n",
       "      <th>feature_46</th>\n",
       "      <th>feature_47</th>\n",
       "      <th>feature_48</th>\n",
       "      <th>feature_49</th>\n",
       "      <th>feature_50</th>\n",
       "      <th>feature_51</th>\n",
       "      <th>feature_52</th>\n",
       "      <th>feature_53</th>\n",
       "      <th>feature_54</th>\n",
       "      <th>feature_55</th>\n",
       "      <th>feature_56</th>\n",
       "      <th>feature_57</th>\n",
       "      <th>feature_58</th>\n",
       "      <th>feature_59</th>\n",
       "      <th>feature_60</th>\n",
       "      <th>feature_61</th>\n",
       "      <th>feature_62</th>\n",
       "      <th>feature_63</th>\n",
       "      <th>feature_64</th>\n",
       "      <th>feature_65</th>\n",
       "      <th>feature_66</th>\n",
       "      <th>feature_67</th>\n",
       "      <th>feature_68</th>\n",
       "      <th>feature_69</th>\n",
       "      <th>feature_70</th>\n",
       "      <th>feature_71</th>\n",
       "      <th>feature_72</th>\n",
       "      <th>feature_73</th>\n",
       "      <th>feature_74</th>\n",
       "      <th>feature_75</th>\n",
       "      <th>feature_76</th>\n",
       "      <th>feature_77</th>\n",
       "      <th>feature_78</th>\n",
       "      <th>feature_79</th>\n",
       "      <th>feature_80</th>\n",
       "      <th>feature_81</th>\n",
       "      <th>feature_82</th>\n",
       "      <th>feature_83</th>\n",
       "      <th>feature_84</th>\n",
       "      <th>feature_85</th>\n",
       "      <th>feature_86</th>\n",
       "      <th>feature_87</th>\n",
       "      <th>feature_88</th>\n",
       "      <th>feature_89</th>\n",
       "      <th>feature_90</th>\n",
       "      <th>feature_91</th>\n",
       "      <th>feature_92</th>\n",
       "      <th>feature_93</th>\n",
       "      <th>feature_94</th>\n",
       "      <th>feature_95</th>\n",
       "      <th>feature_96</th>\n",
       "      <th>feature_97</th>\n",
       "      <th>feature_98</th>\n",
       "      <th>feature_99</th>\n",
       "      <th>feature_100</th>\n",
       "      <th>feature_101</th>\n",
       "      <th>feature_102</th>\n",
       "      <th>feature_103</th>\n",
       "      <th>feature_104</th>\n",
       "      <th>feature_105</th>\n",
       "      <th>feature_106</th>\n",
       "      <th>feature_107</th>\n",
       "      <th>feature_108</th>\n",
       "      <th>feature_109</th>\n",
       "      <th>feature_110</th>\n",
       "      <th>feature_111</th>\n",
       "      <th>feature_112</th>\n",
       "      <th>feature_113</th>\n",
       "      <th>feature_114</th>\n",
       "      <th>feature_115</th>\n",
       "      <th>feature_116</th>\n",
       "      <th>feature_117</th>\n",
       "      <th>feature_118</th>\n",
       "      <th>feature_119</th>\n",
       "      <th>feature_120</th>\n",
       "      <th>feature_121</th>\n",
       "      <th>feature_122</th>\n",
       "      <th>feature_123</th>\n",
       "      <th>feature_124</th>\n",
       "      <th>feature_125</th>\n",
       "      <th>feature_126</th>\n",
       "      <th>feature_127</th>\n",
       "      <th>feature_128</th>\n",
       "      <th>feature_129</th>\n",
       "      <th>resp_1</th>\n",
       "      <th>resp_2</th>\n",
       "      <th>resp_3</th>\n",
       "      <th>resp_4</th>\n",
       "      <th>resp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.713839</td>\n",
       "      <td>-0.268261</td>\n",
       "      <td>-0.486405</td>\n",
       "      <td>-0.565431</td>\n",
       "      <td>-1.148021</td>\n",
       "      <td>-1.502452</td>\n",
       "      <td>0.041933</td>\n",
       "      <td>0.023107</td>\n",
       "      <td>-1.402398</td>\n",
       "      <td>-0.741073</td>\n",
       "      <td>-1.418300</td>\n",
       "      <td>-2.797995</td>\n",
       "      <td>0.466426</td>\n",
       "      <td>0.914227</td>\n",
       "      <td>-1.079817</td>\n",
       "      <td>-1.288776</td>\n",
       "      <td>0.124517</td>\n",
       "      <td>0.099811</td>\n",
       "      <td>-0.422740</td>\n",
       "      <td>-0.679221</td>\n",
       "      <td>-2.919710</td>\n",
       "      <td>-2.319005</td>\n",
       "      <td>-0.357085</td>\n",
       "      <td>-0.609225</td>\n",
       "      <td>-0.483308</td>\n",
       "      <td>-0.733138</td>\n",
       "      <td>0.140639</td>\n",
       "      <td>0.180904</td>\n",
       "      <td>0.388158</td>\n",
       "      <td>0.738679</td>\n",
       "      <td>0.685296</td>\n",
       "      <td>1.573227</td>\n",
       "      <td>1.143085</td>\n",
       "      <td>1.807628</td>\n",
       "      <td>0.686923</td>\n",
       "      <td>0.975338</td>\n",
       "      <td>-0.782771</td>\n",
       "      <td>-1.084148</td>\n",
       "      <td>0.197984</td>\n",
       "      <td>0.432715</td>\n",
       "      <td>8.940586</td>\n",
       "      <td>-3.204353</td>\n",
       "      <td>-0.918633</td>\n",
       "      <td>-0.501938</td>\n",
       "      <td>1.255588</td>\n",
       "      <td>-0.096371</td>\n",
       "      <td>-0.884003</td>\n",
       "      <td>-0.581895</td>\n",
       "      <td>-0.291385</td>\n",
       "      <td>1.912954</td>\n",
       "      <td>5.609293</td>\n",
       "      <td>2.337896</td>\n",
       "      <td>-2.419713</td>\n",
       "      <td>-0.190391</td>\n",
       "      <td>0.434431</td>\n",
       "      <td>0.373225</td>\n",
       "      <td>1.251686</td>\n",
       "      <td>2.050188</td>\n",
       "      <td>1.149365</td>\n",
       "      <td>-1.033412</td>\n",
       "      <td>-0.866227</td>\n",
       "      <td>-1.146050</td>\n",
       "      <td>-1.237420</td>\n",
       "      <td>-2.839077</td>\n",
       "      <td>-1.236160</td>\n",
       "      <td>-0.857475</td>\n",
       "      <td>-1.580923</td>\n",
       "      <td>-1.520412</td>\n",
       "      <td>-0.754977</td>\n",
       "      <td>3.460446</td>\n",
       "      <td>-0.181478</td>\n",
       "      <td>0.118366</td>\n",
       "      <td>0.222541</td>\n",
       "      <td>-0.056285</td>\n",
       "      <td>0.542955</td>\n",
       "      <td>0.158316</td>\n",
       "      <td>0.124693</td>\n",
       "      <td>0.141826</td>\n",
       "      <td>0.638040</td>\n",
       "      <td>-0.406989</td>\n",
       "      <td>2.647661</td>\n",
       "      <td>0.423757</td>\n",
       "      <td>-0.188005</td>\n",
       "      <td>1.045944</td>\n",
       "      <td>-0.078240</td>\n",
       "      <td>0.167757</td>\n",
       "      <td>0.307411</td>\n",
       "      <td>-0.320207</td>\n",
       "      <td>-1.662068</td>\n",
       "      <td>1.447023</td>\n",
       "      <td>1.013695</td>\n",
       "      <td>2.658406</td>\n",
       "      <td>2.821055</td>\n",
       "      <td>0.728116</td>\n",
       "      <td>-0.239761</td>\n",
       "      <td>1.068505</td>\n",
       "      <td>-1.338859</td>\n",
       "      <td>0.265117</td>\n",
       "      <td>-0.446759</td>\n",
       "      <td>-0.470246</td>\n",
       "      <td>-1.429855</td>\n",
       "      <td>1.481086</td>\n",
       "      <td>-0.940190</td>\n",
       "      <td>2.453729</td>\n",
       "      <td>0.769094</td>\n",
       "      <td>0.026918</td>\n",
       "      <td>-0.229915</td>\n",
       "      <td>1.185998</td>\n",
       "      <td>-2.311855</td>\n",
       "      <td>0.126439</td>\n",
       "      <td>-0.301784</td>\n",
       "      <td>-1.981169</td>\n",
       "      <td>-2.074370</td>\n",
       "      <td>1.460399</td>\n",
       "      <td>-0.576117</td>\n",
       "      <td>3.239120</td>\n",
       "      <td>2.455254</td>\n",
       "      <td>0.345684</td>\n",
       "      <td>-0.181560</td>\n",
       "      <td>-1.518724</td>\n",
       "      <td>2.199555</td>\n",
       "      <td>-2.148027</td>\n",
       "      <td>0.434164</td>\n",
       "      <td>-1.357129</td>\n",
       "      <td>2.226807</td>\n",
       "      <td>-2.412155</td>\n",
       "      <td>1.438557</td>\n",
       "      <td>-2.180290</td>\n",
       "      <td>1.125590</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.190912</td>\n",
       "      <td>0.790829</td>\n",
       "      <td>-2.461037</td>\n",
       "      <td>-0.901529</td>\n",
       "      <td>-3.480929</td>\n",
       "      <td>-1.375299</td>\n",
       "      <td>-3.243520</td>\n",
       "      <td>-1.832464</td>\n",
       "      <td>-1.814910</td>\n",
       "      <td>-1.600502</td>\n",
       "      <td>6.957473</td>\n",
       "      <td>4.004547</td>\n",
       "      <td>13.815534</td>\n",
       "      <td>5.152293</td>\n",
       "      <td>-0.230075</td>\n",
       "      <td>-1.636256</td>\n",
       "      <td>6.225949</td>\n",
       "      <td>3.438644</td>\n",
       "      <td>1.433599</td>\n",
       "      <td>0.301399</td>\n",
       "      <td>12.724837</td>\n",
       "      <td>4.361032</td>\n",
       "      <td>14.534816</td>\n",
       "      <td>7.344823</td>\n",
       "      <td>2.168402</td>\n",
       "      <td>0.536703</td>\n",
       "      <td>-3.600987</td>\n",
       "      <td>-2.369868</td>\n",
       "      <td>-0.409682</td>\n",
       "      <td>-0.173585</td>\n",
       "      <td>-2.077118</td>\n",
       "      <td>-1.886938</td>\n",
       "      <td>-3.616480</td>\n",
       "      <td>-1.974266</td>\n",
       "      <td>-5.464904</td>\n",
       "      <td>-2.759819</td>\n",
       "      <td>2.975461</td>\n",
       "      <td>1.253825</td>\n",
       "      <td>-4.000735</td>\n",
       "      <td>-2.708780</td>\n",
       "      <td>-0.441402</td>\n",
       "      <td>5.965267</td>\n",
       "      <td>7.189540</td>\n",
       "      <td>4.821633</td>\n",
       "      <td>2.948821</td>\n",
       "      <td>0.056696</td>\n",
       "      <td>0.668514</td>\n",
       "      <td>-0.406570</td>\n",
       "      <td>-0.920571</td>\n",
       "      <td>-1.178911</td>\n",
       "      <td>-1.756426</td>\n",
       "      <td>-2.475101</td>\n",
       "      <td>-0.710827</td>\n",
       "      <td>-3.403333</td>\n",
       "      <td>2.615558</td>\n",
       "      <td>1.556327</td>\n",
       "      <td>2.826118</td>\n",
       "      <td>4.826003</td>\n",
       "      <td>4.374602</td>\n",
       "      <td>1.077196</td>\n",
       "      <td>0.945431</td>\n",
       "      <td>0.450841</td>\n",
       "      <td>0.468238</td>\n",
       "      <td>-1.174110</td>\n",
       "      <td>-1.504037</td>\n",
       "      <td>-1.013058</td>\n",
       "      <td>-0.534313</td>\n",
       "      <td>-0.512625</td>\n",
       "      <td>1.528010</td>\n",
       "      <td>-2.110436</td>\n",
       "      <td>-0.685431</td>\n",
       "      <td>2.374812</td>\n",
       "      <td>-0.858263</td>\n",
       "      <td>-5.495745</td>\n",
       "      <td>-10.861986</td>\n",
       "      <td>-1.669940</td>\n",
       "      <td>5.781310</td>\n",
       "      <td>3.854469</td>\n",
       "      <td>-0.802189</td>\n",
       "      <td>-7.335493</td>\n",
       "      <td>-16.609261</td>\n",
       "      <td>-1.799019</td>\n",
       "      <td>6.107036</td>\n",
       "      <td>10.415768</td>\n",
       "      <td>-0.191082</td>\n",
       "      <td>6.930025</td>\n",
       "      <td>8.215279</td>\n",
       "      <td>1.159629</td>\n",
       "      <td>6.756622</td>\n",
       "      <td>13.120583</td>\n",
       "      <td>-0.428775</td>\n",
       "      <td>9.157347</td>\n",
       "      <td>9.293643</td>\n",
       "      <td>1.050990</td>\n",
       "      <td>7.154756</td>\n",
       "      <td>10.391306</td>\n",
       "      <td>1.105226</td>\n",
       "      <td>12.103820</td>\n",
       "      <td>18.137955</td>\n",
       "      <td>2.259194</td>\n",
       "      <td>5.450634</td>\n",
       "      <td>13.106067</td>\n",
       "      <td>1.068980</td>\n",
       "      <td>13.351768</td>\n",
       "      <td>24.399248</td>\n",
       "      <td>3.920204</td>\n",
       "      <td>6.051712</td>\n",
       "      <td>11.970614</td>\n",
       "      <td>-0.127686</td>\n",
       "      <td>9.439947</td>\n",
       "      <td>13.456003</td>\n",
       "      <td>2.100258</td>\n",
       "      <td>6.827902</td>\n",
       "      <td>12.998483</td>\n",
       "      <td>0.212201</td>\n",
       "      <td>12.938593</td>\n",
       "      <td>19.896826</td>\n",
       "      <td>2.835907</td>\n",
       "      <td>5.679259</td>\n",
       "      <td>1.085337</td>\n",
       "      <td>-3.674326</td>\n",
       "      <td>1.083959</td>\n",
       "      <td>-2.725770</td>\n",
       "      <td>1.406325</td>\n",
       "      <td>-3.446246</td>\n",
       "      <td>2.042017</td>\n",
       "      <td>-2.443828</td>\n",
       "      <td>2.028510</td>\n",
       "      <td>-2.018592</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.213468</td>\n",
       "      <td>-0.815284</td>\n",
       "      <td>-2.265903</td>\n",
       "      <td>-3.097641</td>\n",
       "      <td>3.572474</td>\n",
       "      <td>5.050573</td>\n",
       "      <td>1.044638</td>\n",
       "      <td>1.869663</td>\n",
       "      <td>-0.730077</td>\n",
       "      <td>-0.069317</td>\n",
       "      <td>1.225921</td>\n",
       "      <td>3.054743</td>\n",
       "      <td>1.850094</td>\n",
       "      <td>2.687464</td>\n",
       "      <td>0.578551</td>\n",
       "      <td>1.855240</td>\n",
       "      <td>-0.249863</td>\n",
       "      <td>-0.517693</td>\n",
       "      <td>-0.798098</td>\n",
       "      <td>-1.466990</td>\n",
       "      <td>-0.697048</td>\n",
       "      <td>-0.577760</td>\n",
       "      <td>-0.782237</td>\n",
       "      <td>-1.530976</td>\n",
       "      <td>-0.984706</td>\n",
       "      <td>-1.693660</td>\n",
       "      <td>1.536858</td>\n",
       "      <td>3.319996</td>\n",
       "      <td>0.974509</td>\n",
       "      <td>1.967738</td>\n",
       "      <td>2.254830</td>\n",
       "      <td>5.076706</td>\n",
       "      <td>2.385571</td>\n",
       "      <td>3.866634</td>\n",
       "      <td>2.428045</td>\n",
       "      <td>3.681521</td>\n",
       "      <td>-2.776182</td>\n",
       "      <td>-4.379738</td>\n",
       "      <td>1.790262</td>\n",
       "      <td>4.142057</td>\n",
       "      <td>-0.416711</td>\n",
       "      <td>-0.437310</td>\n",
       "      <td>-1.094015</td>\n",
       "      <td>21.839018</td>\n",
       "      <td>9.529975</td>\n",
       "      <td>0.262711</td>\n",
       "      <td>0.196696</td>\n",
       "      <td>0.564349</td>\n",
       "      <td>1.027359</td>\n",
       "      <td>0.734481</td>\n",
       "      <td>0.465569</td>\n",
       "      <td>-0.208017</td>\n",
       "      <td>-1.216604</td>\n",
       "      <td>-1.345248</td>\n",
       "      <td>0.265881</td>\n",
       "      <td>1.143332</td>\n",
       "      <td>0.211766</td>\n",
       "      <td>1.240016</td>\n",
       "      <td>0.788274</td>\n",
       "      <td>0.169722</td>\n",
       "      <td>0.140487</td>\n",
       "      <td>0.847784</td>\n",
       "      <td>0.881343</td>\n",
       "      <td>-0.299437</td>\n",
       "      <td>0.699097</td>\n",
       "      <td>0.577453</td>\n",
       "      <td>0.286329</td>\n",
       "      <td>0.286735</td>\n",
       "      <td>0.751028</td>\n",
       "      <td>1.859740</td>\n",
       "      <td>1.454680</td>\n",
       "      <td>6.380749</td>\n",
       "      <td>1.764019</td>\n",
       "      <td>11.029699</td>\n",
       "      <td>14.842920</td>\n",
       "      <td>9.296057</td>\n",
       "      <td>-0.722392</td>\n",
       "      <td>6.075707</td>\n",
       "      <td>1.136506</td>\n",
       "      <td>8.630273</td>\n",
       "      <td>13.540418</td>\n",
       "      <td>7.826179</td>\n",
       "      <td>-0.467906</td>\n",
       "      <td>5.973352</td>\n",
       "      <td>3.006416</td>\n",
       "      <td>9.992657</td>\n",
       "      <td>21.388948</td>\n",
       "      <td>16.645857</td>\n",
       "      <td>9.440651</td>\n",
       "      <td>4.688157</td>\n",
       "      <td>2.921638</td>\n",
       "      <td>7.577065</td>\n",
       "      <td>15.069448</td>\n",
       "      <td>13.401203</td>\n",
       "      <td>5.992865</td>\n",
       "      <td>3.384810</td>\n",
       "      <td>0.899094</td>\n",
       "      <td>1.203710</td>\n",
       "      <td>1.601242</td>\n",
       "      <td>2.654372</td>\n",
       "      <td>8.942898</td>\n",
       "      <td>2.796522</td>\n",
       "      <td>0.213163</td>\n",
       "      <td>0.396789</td>\n",
       "      <td>1.097522</td>\n",
       "      <td>2.708060</td>\n",
       "      <td>5.802994</td>\n",
       "      <td>5.888627</td>\n",
       "      <td>2.061011</td>\n",
       "      <td>6.002925</td>\n",
       "      <td>11.388774</td>\n",
       "      <td>11.924448</td>\n",
       "      <td>10.030302</td>\n",
       "      <td>3.716764</td>\n",
       "      <td>1.490534</td>\n",
       "      <td>5.202312</td>\n",
       "      <td>10.177697</td>\n",
       "      <td>9.881399</td>\n",
       "      <td>5.033299</td>\n",
       "      <td>-0.673786</td>\n",
       "      <td>-0.312518</td>\n",
       "      <td>-0.486230</td>\n",
       "      <td>0.221370</td>\n",
       "      <td>-0.563331</td>\n",
       "      <td>-0.219143</td>\n",
       "      <td>-0.876764</td>\n",
       "      <td>-0.233487</td>\n",
       "      <td>-0.747413</td>\n",
       "      <td>-0.163139</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.998833</td>\n",
       "      <td>0.040604</td>\n",
       "      <td>-2.680824</td>\n",
       "      <td>-1.877893</td>\n",
       "      <td>-1.924607</td>\n",
       "      <td>-1.318314</td>\n",
       "      <td>-0.996132</td>\n",
       "      <td>-0.898262</td>\n",
       "      <td>-0.714493</td>\n",
       "      <td>-1.023592</td>\n",
       "      <td>-1.240550</td>\n",
       "      <td>-1.701951</td>\n",
       "      <td>-1.276781</td>\n",
       "      <td>-1.107550</td>\n",
       "      <td>-1.460069</td>\n",
       "      <td>-2.009983</td>\n",
       "      <td>0.073626</td>\n",
       "      <td>0.002253</td>\n",
       "      <td>0.499573</td>\n",
       "      <td>0.213440</td>\n",
       "      <td>0.239066</td>\n",
       "      <td>0.036540</td>\n",
       "      <td>0.348198</td>\n",
       "      <td>0.111449</td>\n",
       "      <td>0.528970</td>\n",
       "      <td>0.188996</td>\n",
       "      <td>-0.883288</td>\n",
       "      <td>-1.048934</td>\n",
       "      <td>-1.145531</td>\n",
       "      <td>-1.070988</td>\n",
       "      <td>-1.135388</td>\n",
       "      <td>-1.621302</td>\n",
       "      <td>-0.917998</td>\n",
       "      <td>-0.610962</td>\n",
       "      <td>-1.737500</td>\n",
       "      <td>-1.060114</td>\n",
       "      <td>0.012030</td>\n",
       "      <td>-0.017236</td>\n",
       "      <td>-3.015418</td>\n",
       "      <td>-3.628733</td>\n",
       "      <td>-0.607337</td>\n",
       "      <td>1.699008</td>\n",
       "      <td>1.992606</td>\n",
       "      <td>-3.104658</td>\n",
       "      <td>-0.980927</td>\n",
       "      <td>0.720345</td>\n",
       "      <td>1.286729</td>\n",
       "      <td>0.709237</td>\n",
       "      <td>0.229676</td>\n",
       "      <td>0.313247</td>\n",
       "      <td>-0.362713</td>\n",
       "      <td>-0.714402</td>\n",
       "      <td>-0.552775</td>\n",
       "      <td>-2.174973</td>\n",
       "      <td>-0.326162</td>\n",
       "      <td>-0.168435</td>\n",
       "      <td>-0.990813</td>\n",
       "      <td>-0.736768</td>\n",
       "      <td>-0.326698</td>\n",
       "      <td>4.542076</td>\n",
       "      <td>4.201020</td>\n",
       "      <td>-0.227355</td>\n",
       "      <td>-0.247617</td>\n",
       "      <td>1.894440</td>\n",
       "      <td>-1.909655</td>\n",
       "      <td>-1.546615</td>\n",
       "      <td>3.449815</td>\n",
       "      <td>3.398122</td>\n",
       "      <td>0.286515</td>\n",
       "      <td>-1.620329</td>\n",
       "      <td>-0.484429</td>\n",
       "      <td>0.281887</td>\n",
       "      <td>-1.360754</td>\n",
       "      <td>0.345321</td>\n",
       "      <td>-0.804103</td>\n",
       "      <td>-1.307855</td>\n",
       "      <td>1.038428</td>\n",
       "      <td>0.451391</td>\n",
       "      <td>-1.149930</td>\n",
       "      <td>0.371895</td>\n",
       "      <td>-1.025794</td>\n",
       "      <td>-1.089371</td>\n",
       "      <td>1.002081</td>\n",
       "      <td>-0.121205</td>\n",
       "      <td>-1.238212</td>\n",
       "      <td>-0.028325</td>\n",
       "      <td>-2.321156</td>\n",
       "      <td>-1.641860</td>\n",
       "      <td>0.392638</td>\n",
       "      <td>-0.357487</td>\n",
       "      <td>-1.515613</td>\n",
       "      <td>-0.215753</td>\n",
       "      <td>-1.746285</td>\n",
       "      <td>-1.086886</td>\n",
       "      <td>0.293863</td>\n",
       "      <td>-0.373109</td>\n",
       "      <td>1.769451</td>\n",
       "      <td>-0.606460</td>\n",
       "      <td>0.250006</td>\n",
       "      <td>1.094486</td>\n",
       "      <td>0.177932</td>\n",
       "      <td>-0.707806</td>\n",
       "      <td>1.488209</td>\n",
       "      <td>-0.552591</td>\n",
       "      <td>0.097511</td>\n",
       "      <td>1.388289</td>\n",
       "      <td>0.128598</td>\n",
       "      <td>-0.455873</td>\n",
       "      <td>0.494581</td>\n",
       "      <td>-0.303207</td>\n",
       "      <td>-0.411745</td>\n",
       "      <td>0.134636</td>\n",
       "      <td>0.338933</td>\n",
       "      <td>-0.523544</td>\n",
       "      <td>0.537610</td>\n",
       "      <td>-0.670995</td>\n",
       "      <td>-1.014795</td>\n",
       "      <td>0.077621</td>\n",
       "      <td>0.151990</td>\n",
       "      <td>0.317614</td>\n",
       "      <td>-1.925987</td>\n",
       "      <td>-0.613339</td>\n",
       "      <td>-2.731776</td>\n",
       "      <td>0.154530</td>\n",
       "      <td>-2.430828</td>\n",
       "      <td>0.125178</td>\n",
       "      <td>-1.879093</td>\n",
       "      <td>-0.037551</td>\n",
       "      <td>-1.821756</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.033963</td>\n",
       "      <td>-0.187102</td>\n",
       "      <td>0.058072</td>\n",
       "      <td>-0.009353</td>\n",
       "      <td>0.003324</td>\n",
       "      <td>-0.015159</td>\n",
       "      <td>-1.483324</td>\n",
       "      <td>-1.224971</td>\n",
       "      <td>2.316799</td>\n",
       "      <td>0.561196</td>\n",
       "      <td>-1.527731</td>\n",
       "      <td>-1.923415</td>\n",
       "      <td>1.424692</td>\n",
       "      <td>0.314437</td>\n",
       "      <td>1.587393</td>\n",
       "      <td>0.643946</td>\n",
       "      <td>0.569870</td>\n",
       "      <td>0.306798</td>\n",
       "      <td>2.292751</td>\n",
       "      <td>1.577470</td>\n",
       "      <td>1.492811</td>\n",
       "      <td>0.471517</td>\n",
       "      <td>1.826803</td>\n",
       "      <td>1.208687</td>\n",
       "      <td>2.463869</td>\n",
       "      <td>1.607891</td>\n",
       "      <td>-2.535065</td>\n",
       "      <td>-2.612160</td>\n",
       "      <td>-2.687100</td>\n",
       "      <td>-2.592639</td>\n",
       "      <td>-1.457048</td>\n",
       "      <td>-1.906608</td>\n",
       "      <td>-2.043063</td>\n",
       "      <td>-1.556274</td>\n",
       "      <td>-3.823113</td>\n",
       "      <td>-2.605627</td>\n",
       "      <td>1.686712</td>\n",
       "      <td>1.106271</td>\n",
       "      <td>-2.798567</td>\n",
       "      <td>-3.037946</td>\n",
       "      <td>-1.822323</td>\n",
       "      <td>4.370279</td>\n",
       "      <td>2.669122</td>\n",
       "      <td>-5.198174</td>\n",
       "      <td>-2.061702</td>\n",
       "      <td>1.779870</td>\n",
       "      <td>3.148127</td>\n",
       "      <td>2.386779</td>\n",
       "      <td>1.285549</td>\n",
       "      <td>0.487389</td>\n",
       "      <td>-0.668858</td>\n",
       "      <td>0.039408</td>\n",
       "      <td>2.738650</td>\n",
       "      <td>-0.565351</td>\n",
       "      <td>1.404716</td>\n",
       "      <td>-0.215895</td>\n",
       "      <td>0.803014</td>\n",
       "      <td>0.874106</td>\n",
       "      <td>-0.656432</td>\n",
       "      <td>5.956661</td>\n",
       "      <td>5.504277</td>\n",
       "      <td>4.408381</td>\n",
       "      <td>4.520513</td>\n",
       "      <td>3.393953</td>\n",
       "      <td>-0.779575</td>\n",
       "      <td>-0.567095</td>\n",
       "      <td>0.944644</td>\n",
       "      <td>0.931463</td>\n",
       "      <td>0.316043</td>\n",
       "      <td>-2.415911</td>\n",
       "      <td>-0.635696</td>\n",
       "      <td>-0.854428</td>\n",
       "      <td>-1.860926</td>\n",
       "      <td>-0.608877</td>\n",
       "      <td>-1.128558</td>\n",
       "      <td>-1.793292</td>\n",
       "      <td>1.765971</td>\n",
       "      <td>-0.720645</td>\n",
       "      <td>-0.879999</td>\n",
       "      <td>-0.337626</td>\n",
       "      <td>-0.776670</td>\n",
       "      <td>-0.829852</td>\n",
       "      <td>1.075422</td>\n",
       "      <td>0.379688</td>\n",
       "      <td>-1.238212</td>\n",
       "      <td>-1.294593</td>\n",
       "      <td>-2.321156</td>\n",
       "      <td>-1.641860</td>\n",
       "      <td>1.379883</td>\n",
       "      <td>-0.556350</td>\n",
       "      <td>-1.515613</td>\n",
       "      <td>-2.115890</td>\n",
       "      <td>-1.746285</td>\n",
       "      <td>-1.086886</td>\n",
       "      <td>0.100359</td>\n",
       "      <td>0.814115</td>\n",
       "      <td>2.541056</td>\n",
       "      <td>-0.184360</td>\n",
       "      <td>0.635247</td>\n",
       "      <td>1.691754</td>\n",
       "      <td>0.980007</td>\n",
       "      <td>-0.312455</td>\n",
       "      <td>1.053210</td>\n",
       "      <td>-0.709496</td>\n",
       "      <td>-0.150937</td>\n",
       "      <td>0.887753</td>\n",
       "      <td>-0.094194</td>\n",
       "      <td>0.890213</td>\n",
       "      <td>1.260494</td>\n",
       "      <td>-0.700629</td>\n",
       "      <td>-0.079456</td>\n",
       "      <td>0.858195</td>\n",
       "      <td>1.453121</td>\n",
       "      <td>-0.451007</td>\n",
       "      <td>0.026876</td>\n",
       "      <td>-2.244573</td>\n",
       "      <td>-1.466553</td>\n",
       "      <td>-0.398128</td>\n",
       "      <td>-0.003867</td>\n",
       "      <td>3.126824</td>\n",
       "      <td>-0.452267</td>\n",
       "      <td>2.582727</td>\n",
       "      <td>-1.027463</td>\n",
       "      <td>3.832805</td>\n",
       "      <td>0.636070</td>\n",
       "      <td>4.585393</td>\n",
       "      <td>-0.029508</td>\n",
       "      <td>2.826941</td>\n",
       "      <td>-1.046821</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>10.140003</td>\n",
       "      <td>7.017232</td>\n",
       "      <td>1.494698</td>\n",
       "      <td>0.805080</td>\n",
       "      <td>0.967548</td>\n",
       "      <td>0.515505</td>\n",
       "      <td>3.511944</td>\n",
       "      <td>2.742482</td>\n",
       "      <td>9.444735</td>\n",
       "      <td>5.027342</td>\n",
       "      <td>2.233371</td>\n",
       "      <td>2.158356</td>\n",
       "      <td>5.682027</td>\n",
       "      <td>3.348379</td>\n",
       "      <td>7.015429</td>\n",
       "      <td>5.427101</td>\n",
       "      <td>3.830684</td>\n",
       "      <td>3.487000</td>\n",
       "      <td>8.154271</td>\n",
       "      <td>6.284466</td>\n",
       "      <td>5.061613</td>\n",
       "      <td>2.535438</td>\n",
       "      <td>5.955424</td>\n",
       "      <td>5.125734</td>\n",
       "      <td>8.181330</td>\n",
       "      <td>6.320902</td>\n",
       "      <td>-0.545506</td>\n",
       "      <td>-0.614559</td>\n",
       "      <td>-2.519999</td>\n",
       "      <td>-2.454581</td>\n",
       "      <td>-1.077958</td>\n",
       "      <td>-1.438075</td>\n",
       "      <td>-1.932549</td>\n",
       "      <td>-1.476723</td>\n",
       "      <td>-3.626845</td>\n",
       "      <td>-2.471910</td>\n",
       "      <td>2.617152</td>\n",
       "      <td>1.906341</td>\n",
       "      <td>-0.193643</td>\n",
       "      <td>-0.234820</td>\n",
       "      <td>-2.130381</td>\n",
       "      <td>5.046810</td>\n",
       "      <td>2.562002</td>\n",
       "      <td>-5.786360</td>\n",
       "      <td>-2.227558</td>\n",
       "      <td>2.473904</td>\n",
       "      <td>3.479277</td>\n",
       "      <td>2.841379</td>\n",
       "      <td>1.640252</td>\n",
       "      <td>0.758055</td>\n",
       "      <td>-0.628700</td>\n",
       "      <td>-2.309391</td>\n",
       "      <td>3.746835</td>\n",
       "      <td>-0.076208</td>\n",
       "      <td>-0.538131</td>\n",
       "      <td>0.217494</td>\n",
       "      <td>-0.251795</td>\n",
       "      <td>-0.963646</td>\n",
       "      <td>-1.202921</td>\n",
       "      <td>-1.293512</td>\n",
       "      <td>-1.080720</td>\n",
       "      <td>1.216589</td>\n",
       "      <td>1.262354</td>\n",
       "      <td>-1.074454</td>\n",
       "      <td>0.238713</td>\n",
       "      <td>0.197790</td>\n",
       "      <td>-2.768644</td>\n",
       "      <td>-2.616471</td>\n",
       "      <td>12.017917</td>\n",
       "      <td>4.546630</td>\n",
       "      <td>7.270123</td>\n",
       "      <td>-2.797911</td>\n",
       "      <td>-6.625528</td>\n",
       "      <td>-2.479670</td>\n",
       "      <td>-4.216096</td>\n",
       "      <td>-6.251039</td>\n",
       "      <td>-2.473847</td>\n",
       "      <td>-2.448277</td>\n",
       "      <td>-4.187983</td>\n",
       "      <td>-1.641606</td>\n",
       "      <td>-3.341024</td>\n",
       "      <td>-3.832051</td>\n",
       "      <td>-1.343812</td>\n",
       "      <td>-1.751180</td>\n",
       "      <td>-1.238212</td>\n",
       "      <td>-1.294593</td>\n",
       "      <td>-2.321156</td>\n",
       "      <td>-1.641860</td>\n",
       "      <td>-1.525782</td>\n",
       "      <td>-2.937308</td>\n",
       "      <td>-1.515613</td>\n",
       "      <td>-2.115890</td>\n",
       "      <td>-1.746285</td>\n",
       "      <td>-1.086886</td>\n",
       "      <td>-2.524766</td>\n",
       "      <td>1.429724</td>\n",
       "      <td>10.388893</td>\n",
       "      <td>2.310780</td>\n",
       "      <td>4.279600</td>\n",
       "      <td>8.009332</td>\n",
       "      <td>-0.362759</td>\n",
       "      <td>0.336128</td>\n",
       "      <td>6.545966</td>\n",
       "      <td>0.713367</td>\n",
       "      <td>2.389957</td>\n",
       "      <td>6.160709</td>\n",
       "      <td>-1.261344</td>\n",
       "      <td>0.773366</td>\n",
       "      <td>7.528689</td>\n",
       "      <td>0.869695</td>\n",
       "      <td>2.318813</td>\n",
       "      <td>6.258908</td>\n",
       "      <td>-1.049953</td>\n",
       "      <td>-0.510424</td>\n",
       "      <td>4.840134</td>\n",
       "      <td>0.072687</td>\n",
       "      <td>1.673051</td>\n",
       "      <td>3.626101</td>\n",
       "      <td>-1.242035</td>\n",
       "      <td>4.234787</td>\n",
       "      <td>0.068208</td>\n",
       "      <td>3.619194</td>\n",
       "      <td>-0.604773</td>\n",
       "      <td>3.868989</td>\n",
       "      <td>-0.097618</td>\n",
       "      <td>4.788733</td>\n",
       "      <td>-0.441587</td>\n",
       "      <td>3.950345</td>\n",
       "      <td>-0.643430</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.081598</td>\n",
       "      <td>0.931190</td>\n",
       "      <td>-0.456659</td>\n",
       "      <td>-0.563532</td>\n",
       "      <td>-0.738116</td>\n",
       "      <td>-1.015884</td>\n",
       "      <td>0.599169</td>\n",
       "      <td>1.152266</td>\n",
       "      <td>-0.614739</td>\n",
       "      <td>0.087910</td>\n",
       "      <td>-0.613886</td>\n",
       "      <td>-1.097333</td>\n",
       "      <td>-0.553988</td>\n",
       "      <td>-0.203449</td>\n",
       "      <td>-0.400496</td>\n",
       "      <td>0.081490</td>\n",
       "      <td>-0.515536</td>\n",
       "      <td>-0.941414</td>\n",
       "      <td>-0.475866</td>\n",
       "      <td>-0.842720</td>\n",
       "      <td>-0.343494</td>\n",
       "      <td>-0.320303</td>\n",
       "      <td>-0.414915</td>\n",
       "      <td>-0.802837</td>\n",
       "      <td>-0.554066</td>\n",
       "      <td>-0.934320</td>\n",
       "      <td>0.843296</td>\n",
       "      <td>1.917207</td>\n",
       "      <td>0.713104</td>\n",
       "      <td>1.493057</td>\n",
       "      <td>0.567456</td>\n",
       "      <td>1.400717</td>\n",
       "      <td>0.499536</td>\n",
       "      <td>0.837784</td>\n",
       "      <td>0.897439</td>\n",
       "      <td>1.408872</td>\n",
       "      <td>-0.654196</td>\n",
       "      <td>-0.948767</td>\n",
       "      <td>0.431250</td>\n",
       "      <td>1.028566</td>\n",
       "      <td>5.769500</td>\n",
       "      <td>-1.778955</td>\n",
       "      <td>-1.196751</td>\n",
       "      <td>-0.895565</td>\n",
       "      <td>0.521863</td>\n",
       "      <td>-0.753160</td>\n",
       "      <td>-1.874493</td>\n",
       "      <td>-1.417344</td>\n",
       "      <td>-0.999032</td>\n",
       "      <td>-0.433161</td>\n",
       "      <td>1.150818</td>\n",
       "      <td>-2.070659</td>\n",
       "      <td>-0.888264</td>\n",
       "      <td>1.492269</td>\n",
       "      <td>-0.010672</td>\n",
       "      <td>-0.199816</td>\n",
       "      <td>0.230268</td>\n",
       "      <td>-0.668824</td>\n",
       "      <td>-0.103304</td>\n",
       "      <td>-1.610537</td>\n",
       "      <td>-1.350507</td>\n",
       "      <td>0.332265</td>\n",
       "      <td>0.343374</td>\n",
       "      <td>-1.589405</td>\n",
       "      <td>-0.215177</td>\n",
       "      <td>-0.159776</td>\n",
       "      <td>-1.081165</td>\n",
       "      <td>-1.041137</td>\n",
       "      <td>-0.127733</td>\n",
       "      <td>2.780036</td>\n",
       "      <td>0.526967</td>\n",
       "      <td>0.552045</td>\n",
       "      <td>0.598917</td>\n",
       "      <td>0.169574</td>\n",
       "      <td>0.342981</td>\n",
       "      <td>0.566847</td>\n",
       "      <td>-0.039294</td>\n",
       "      <td>2.152907</td>\n",
       "      <td>1.363471</td>\n",
       "      <td>0.598731</td>\n",
       "      <td>1.318223</td>\n",
       "      <td>1.299728</td>\n",
       "      <td>-0.549525</td>\n",
       "      <td>-0.296242</td>\n",
       "      <td>0.558397</td>\n",
       "      <td>-0.527685</td>\n",
       "      <td>-0.538352</td>\n",
       "      <td>0.216252</td>\n",
       "      <td>-1.150923</td>\n",
       "      <td>1.068903</td>\n",
       "      <td>2.680613</td>\n",
       "      <td>0.260541</td>\n",
       "      <td>0.656708</td>\n",
       "      <td>1.304612</td>\n",
       "      <td>0.164602</td>\n",
       "      <td>-0.982152</td>\n",
       "      <td>-1.338859</td>\n",
       "      <td>-1.281433</td>\n",
       "      <td>-0.874411</td>\n",
       "      <td>-1.194013</td>\n",
       "      <td>-0.977244</td>\n",
       "      <td>0.342999</td>\n",
       "      <td>-0.940190</td>\n",
       "      <td>-0.249213</td>\n",
       "      <td>-0.735325</td>\n",
       "      <td>-1.781693</td>\n",
       "      <td>0.214546</td>\n",
       "      <td>-1.082016</td>\n",
       "      <td>-1.070260</td>\n",
       "      <td>-0.932064</td>\n",
       "      <td>-0.923979</td>\n",
       "      <td>-1.471673</td>\n",
       "      <td>-1.409232</td>\n",
       "      <td>0.719524</td>\n",
       "      <td>0.830071</td>\n",
       "      <td>-0.058839</td>\n",
       "      <td>-0.082723</td>\n",
       "      <td>0.344046</td>\n",
       "      <td>0.135843</td>\n",
       "      <td>-1.579855</td>\n",
       "      <td>-0.578825</td>\n",
       "      <td>-1.032219</td>\n",
       "      <td>1.330711</td>\n",
       "      <td>-1.300580</td>\n",
       "      <td>-0.202582</td>\n",
       "      <td>-1.851058</td>\n",
       "      <td>0.434181</td>\n",
       "      <td>-1.481252</td>\n",
       "      <td>0.649619</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.931469</td>\n",
       "      <td>-1.127545</td>\n",
       "      <td>4.027192</td>\n",
       "      <td>4.145205</td>\n",
       "      <td>2.952078</td>\n",
       "      <td>2.975542</td>\n",
       "      <td>0.091784</td>\n",
       "      <td>0.037259</td>\n",
       "      <td>1.000070</td>\n",
       "      <td>0.748420</td>\n",
       "      <td>0.394909</td>\n",
       "      <td>0.565021</td>\n",
       "      <td>0.483711</td>\n",
       "      <td>0.340603</td>\n",
       "      <td>0.540004</td>\n",
       "      <td>0.687382</td>\n",
       "      <td>1.046630</td>\n",
       "      <td>1.417643</td>\n",
       "      <td>1.432864</td>\n",
       "      <td>1.764001</td>\n",
       "      <td>1.130728</td>\n",
       "      <td>0.709177</td>\n",
       "      <td>1.133303</td>\n",
       "      <td>1.376414</td>\n",
       "      <td>1.554012</td>\n",
       "      <td>1.806275</td>\n",
       "      <td>-0.204402</td>\n",
       "      <td>-0.331817</td>\n",
       "      <td>-0.997763</td>\n",
       "      <td>-1.383036</td>\n",
       "      <td>-0.428056</td>\n",
       "      <td>-0.797886</td>\n",
       "      <td>-0.789368</td>\n",
       "      <td>-0.819471</td>\n",
       "      <td>-1.506981</td>\n",
       "      <td>-1.388915</td>\n",
       "      <td>4.489151</td>\n",
       "      <td>5.293788</td>\n",
       "      <td>-0.274084</td>\n",
       "      <td>-0.454352</td>\n",
       "      <td>-0.802129</td>\n",
       "      <td>0.962056</td>\n",
       "      <td>0.296027</td>\n",
       "      <td>-2.751925</td>\n",
       "      <td>-1.021671</td>\n",
       "      <td>0.523978</td>\n",
       "      <td>0.671852</td>\n",
       "      <td>0.609086</td>\n",
       "      <td>0.558003</td>\n",
       "      <td>0.353082</td>\n",
       "      <td>-0.412434</td>\n",
       "      <td>-2.124977</td>\n",
       "      <td>1.609728</td>\n",
       "      <td>1.049996</td>\n",
       "      <td>0.654425</td>\n",
       "      <td>-0.255569</td>\n",
       "      <td>-0.274694</td>\n",
       "      <td>-0.460354</td>\n",
       "      <td>-1.261051</td>\n",
       "      <td>-1.771195</td>\n",
       "      <td>-1.494286</td>\n",
       "      <td>2.284402</td>\n",
       "      <td>2.358915</td>\n",
       "      <td>0.261186</td>\n",
       "      <td>1.522802</td>\n",
       "      <td>1.278796</td>\n",
       "      <td>-0.699432</td>\n",
       "      <td>-0.672568</td>\n",
       "      <td>0.994177</td>\n",
       "      <td>-0.041584</td>\n",
       "      <td>0.797390</td>\n",
       "      <td>-0.949825</td>\n",
       "      <td>-3.430691</td>\n",
       "      <td>-1.223057</td>\n",
       "      <td>-2.147394</td>\n",
       "      <td>-3.288259</td>\n",
       "      <td>-2.780725</td>\n",
       "      <td>-1.161693</td>\n",
       "      <td>-2.745615</td>\n",
       "      <td>-1.111425</td>\n",
       "      <td>-2.341565</td>\n",
       "      <td>-2.561141</td>\n",
       "      <td>-2.023024</td>\n",
       "      <td>-0.658208</td>\n",
       "      <td>-1.238212</td>\n",
       "      <td>-0.733339</td>\n",
       "      <td>-2.321156</td>\n",
       "      <td>-1.641860</td>\n",
       "      <td>-0.331874</td>\n",
       "      <td>-1.247799</td>\n",
       "      <td>-1.515613</td>\n",
       "      <td>-1.297460</td>\n",
       "      <td>-1.746285</td>\n",
       "      <td>-1.086886</td>\n",
       "      <td>-0.738866</td>\n",
       "      <td>0.114286</td>\n",
       "      <td>5.039278</td>\n",
       "      <td>0.963453</td>\n",
       "      <td>1.810452</td>\n",
       "      <td>3.662412</td>\n",
       "      <td>0.393077</td>\n",
       "      <td>-0.516680</td>\n",
       "      <td>4.109815</td>\n",
       "      <td>0.289598</td>\n",
       "      <td>1.378046</td>\n",
       "      <td>4.012724</td>\n",
       "      <td>-0.043409</td>\n",
       "      <td>-0.441653</td>\n",
       "      <td>3.420112</td>\n",
       "      <td>0.146566</td>\n",
       "      <td>0.774099</td>\n",
       "      <td>2.788050</td>\n",
       "      <td>0.089823</td>\n",
       "      <td>-0.841762</td>\n",
       "      <td>2.945290</td>\n",
       "      <td>-0.341043</td>\n",
       "      <td>0.687224</td>\n",
       "      <td>2.140892</td>\n",
       "      <td>-0.262328</td>\n",
       "      <td>0.759471</td>\n",
       "      <td>-0.154235</td>\n",
       "      <td>1.445814</td>\n",
       "      <td>0.614545</td>\n",
       "      <td>0.720087</td>\n",
       "      <td>-0.195408</td>\n",
       "      <td>1.395597</td>\n",
       "      <td>0.214974</td>\n",
       "      <td>1.540928</td>\n",
       "      <td>0.482470</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.464570</td>\n",
       "      <td>-1.296721</td>\n",
       "      <td>-1.390741</td>\n",
       "      <td>-0.859604</td>\n",
       "      <td>0.099606</td>\n",
       "      <td>0.039828</td>\n",
       "      <td>11.280120</td>\n",
       "      <td>9.151466</td>\n",
       "      <td>3.141984</td>\n",
       "      <td>1.314092</td>\n",
       "      <td>3.317384</td>\n",
       "      <td>3.556642</td>\n",
       "      <td>8.389960</td>\n",
       "      <td>5.583041</td>\n",
       "      <td>6.714979</td>\n",
       "      <td>5.653121</td>\n",
       "      <td>12.621574</td>\n",
       "      <td>10.943199</td>\n",
       "      <td>2.884104</td>\n",
       "      <td>2.342577</td>\n",
       "      <td>7.652305</td>\n",
       "      <td>4.445764</td>\n",
       "      <td>9.206506</td>\n",
       "      <td>8.208826</td>\n",
       "      <td>7.954718</td>\n",
       "      <td>6.632580</td>\n",
       "      <td>-0.112239</td>\n",
       "      <td>-0.067653</td>\n",
       "      <td>-0.651724</td>\n",
       "      <td>-0.502712</td>\n",
       "      <td>-0.244916</td>\n",
       "      <td>-0.184685</td>\n",
       "      <td>-0.472787</td>\n",
       "      <td>-0.232519</td>\n",
       "      <td>-0.942734</td>\n",
       "      <td>-0.458979</td>\n",
       "      <td>6.738147</td>\n",
       "      <td>5.695884</td>\n",
       "      <td>-3.364795</td>\n",
       "      <td>-3.964795</td>\n",
       "      <td>-0.826750</td>\n",
       "      <td>3.798993</td>\n",
       "      <td>2.094465</td>\n",
       "      <td>-0.323088</td>\n",
       "      <td>-0.257030</td>\n",
       "      <td>0.048102</td>\n",
       "      <td>-0.086701</td>\n",
       "      <td>-0.444589</td>\n",
       "      <td>-0.631068</td>\n",
       "      <td>-0.863241</td>\n",
       "      <td>-1.776317</td>\n",
       "      <td>-1.509645</td>\n",
       "      <td>0.508599</td>\n",
       "      <td>-2.060036</td>\n",
       "      <td>1.755964</td>\n",
       "      <td>1.563914</td>\n",
       "      <td>2.150128</td>\n",
       "      <td>1.011432</td>\n",
       "      <td>2.300969</td>\n",
       "      <td>5.735872</td>\n",
       "      <td>5.301530</td>\n",
       "      <td>2.846458</td>\n",
       "      <td>2.934365</td>\n",
       "      <td>3.365805</td>\n",
       "      <td>-0.501764</td>\n",
       "      <td>-0.372763</td>\n",
       "      <td>2.558616</td>\n",
       "      <td>2.521679</td>\n",
       "      <td>1.573673</td>\n",
       "      <td>-1.488485</td>\n",
       "      <td>0.415079</td>\n",
       "      <td>-7.893088</td>\n",
       "      <td>-1.215148</td>\n",
       "      <td>-0.340400</td>\n",
       "      <td>-3.980270</td>\n",
       "      <td>-4.328348</td>\n",
       "      <td>-5.654884</td>\n",
       "      <td>-9.109919</td>\n",
       "      <td>-0.969261</td>\n",
       "      <td>-0.349169</td>\n",
       "      <td>-5.033989</td>\n",
       "      <td>-4.311641</td>\n",
       "      <td>-4.904261</td>\n",
       "      <td>3.974446</td>\n",
       "      <td>-1.238212</td>\n",
       "      <td>2.648462</td>\n",
       "      <td>-0.262748</td>\n",
       "      <td>-0.161263</td>\n",
       "      <td>4.502173</td>\n",
       "      <td>4.226622</td>\n",
       "      <td>-1.515613</td>\n",
       "      <td>2.904492</td>\n",
       "      <td>-0.466991</td>\n",
       "      <td>-0.372457</td>\n",
       "      <td>3.866725</td>\n",
       "      <td>8.257419</td>\n",
       "      <td>1.546203</td>\n",
       "      <td>3.452826</td>\n",
       "      <td>4.183638</td>\n",
       "      <td>5.255452</td>\n",
       "      <td>5.023695</td>\n",
       "      <td>8.440703</td>\n",
       "      <td>1.197024</td>\n",
       "      <td>2.661573</td>\n",
       "      <td>4.372510</td>\n",
       "      <td>7.113380</td>\n",
       "      <td>4.488747</td>\n",
       "      <td>7.349967</td>\n",
       "      <td>0.255781</td>\n",
       "      <td>2.841606</td>\n",
       "      <td>2.374830</td>\n",
       "      <td>4.224848</td>\n",
       "      <td>5.441684</td>\n",
       "      <td>6.203197</td>\n",
       "      <td>0.202196</td>\n",
       "      <td>3.506107</td>\n",
       "      <td>3.402036</td>\n",
       "      <td>4.370002</td>\n",
       "      <td>3.480856</td>\n",
       "      <td>2.297857</td>\n",
       "      <td>-1.018130</td>\n",
       "      <td>1.283170</td>\n",
       "      <td>-1.846075</td>\n",
       "      <td>1.931890</td>\n",
       "      <td>-1.527131</td>\n",
       "      <td>2.044319</td>\n",
       "      <td>-1.675572</td>\n",
       "      <td>1.435627</td>\n",
       "      <td>-1.775380</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.073642</td>\n",
       "      <td>2.757308</td>\n",
       "      <td>-4.212242</td>\n",
       "      <td>-2.585286</td>\n",
       "      <td>-3.790503</td>\n",
       "      <td>-2.291679</td>\n",
       "      <td>2.536997</td>\n",
       "      <td>1.710952</td>\n",
       "      <td>2.571431</td>\n",
       "      <td>0.525129</td>\n",
       "      <td>-1.118389</td>\n",
       "      <td>-1.446290</td>\n",
       "      <td>-2.505882</td>\n",
       "      <td>-1.670231</td>\n",
       "      <td>0.974300</td>\n",
       "      <td>-0.161780</td>\n",
       "      <td>2.960016</td>\n",
       "      <td>2.432946</td>\n",
       "      <td>2.471396</td>\n",
       "      <td>1.541895</td>\n",
       "      <td>1.922326</td>\n",
       "      <td>0.599002</td>\n",
       "      <td>2.338442</td>\n",
       "      <td>1.515980</td>\n",
       "      <td>3.147554</td>\n",
       "      <td>1.970130</td>\n",
       "      <td>-1.657198</td>\n",
       "      <td>-1.645534</td>\n",
       "      <td>-0.804507</td>\n",
       "      <td>-0.508153</td>\n",
       "      <td>-2.577738</td>\n",
       "      <td>-2.953975</td>\n",
       "      <td>-1.735942</td>\n",
       "      <td>-1.190849</td>\n",
       "      <td>-1.196060</td>\n",
       "      <td>-0.464678</td>\n",
       "      <td>4.830804</td>\n",
       "      <td>3.445934</td>\n",
       "      <td>-3.663632</td>\n",
       "      <td>-3.674105</td>\n",
       "      <td>0.902667</td>\n",
       "      <td>0.251842</td>\n",
       "      <td>3.199941</td>\n",
       "      <td>2.131977</td>\n",
       "      <td>3.181515</td>\n",
       "      <td>-0.205249</td>\n",
       "      <td>-0.364144</td>\n",
       "      <td>-0.785082</td>\n",
       "      <td>-0.957073</td>\n",
       "      <td>-0.363825</td>\n",
       "      <td>-0.297378</td>\n",
       "      <td>-3.060830</td>\n",
       "      <td>0.145786</td>\n",
       "      <td>0.093193</td>\n",
       "      <td>1.814716</td>\n",
       "      <td>0.486196</td>\n",
       "      <td>1.053949</td>\n",
       "      <td>1.003138</td>\n",
       "      <td>1.356292</td>\n",
       "      <td>5.442850</td>\n",
       "      <td>5.029592</td>\n",
       "      <td>-0.686804</td>\n",
       "      <td>-0.740447</td>\n",
       "      <td>4.215258</td>\n",
       "      <td>0.566613</td>\n",
       "      <td>0.467086</td>\n",
       "      <td>6.308042</td>\n",
       "      <td>6.190354</td>\n",
       "      <td>3.936513</td>\n",
       "      <td>4.796611</td>\n",
       "      <td>1.507610</td>\n",
       "      <td>-2.847799</td>\n",
       "      <td>-1.288204</td>\n",
       "      <td>-0.489769</td>\n",
       "      <td>0.281079</td>\n",
       "      <td>-0.935844</td>\n",
       "      <td>-1.954208</td>\n",
       "      <td>-9.377035</td>\n",
       "      <td>-3.241803</td>\n",
       "      <td>-1.573052</td>\n",
       "      <td>1.062415</td>\n",
       "      <td>-2.260557</td>\n",
       "      <td>-4.097869</td>\n",
       "      <td>2.878095</td>\n",
       "      <td>0.117250</td>\n",
       "      <td>0.924313</td>\n",
       "      <td>2.044226</td>\n",
       "      <td>1.048406</td>\n",
       "      <td>7.698643</td>\n",
       "      <td>9.722963</td>\n",
       "      <td>1.069840</td>\n",
       "      <td>3.773966</td>\n",
       "      <td>5.223929</td>\n",
       "      <td>3.085646</td>\n",
       "      <td>20.071367</td>\n",
       "      <td>4.283295</td>\n",
       "      <td>1.865643</td>\n",
       "      <td>1.545685</td>\n",
       "      <td>1.163254</td>\n",
       "      <td>1.456593</td>\n",
       "      <td>7.361871</td>\n",
       "      <td>13.980173</td>\n",
       "      <td>5.586401</td>\n",
       "      <td>4.043000</td>\n",
       "      <td>4.422306</td>\n",
       "      <td>6.368703</td>\n",
       "      <td>19.760223</td>\n",
       "      <td>4.573020</td>\n",
       "      <td>0.795111</td>\n",
       "      <td>1.060311</td>\n",
       "      <td>1.164252</td>\n",
       "      <td>1.330048</td>\n",
       "      <td>8.317006</td>\n",
       "      <td>11.699048</td>\n",
       "      <td>4.590377</td>\n",
       "      <td>4.813597</td>\n",
       "      <td>6.047029</td>\n",
       "      <td>5.442545</td>\n",
       "      <td>18.447794</td>\n",
       "      <td>-0.317366</td>\n",
       "      <td>-0.778399</td>\n",
       "      <td>-0.189923</td>\n",
       "      <td>-0.310370</td>\n",
       "      <td>-0.201591</td>\n",
       "      <td>-0.559270</td>\n",
       "      <td>-0.385621</td>\n",
       "      <td>-0.621773</td>\n",
       "      <td>-0.363895</td>\n",
       "      <td>-0.604762</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DLs = TP.dataloaders (bs=BS)\n",
    "DLs.show_batch ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resp_1</th>\n",
       "      <th>resp_2</th>\n",
       "      <th>resp_3</th>\n",
       "      <th>resp_4</th>\n",
       "      <th>resp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4199</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4285</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      resp_1  resp_2  resp_3  resp_4  resp\n",
       "4199     0.0     1.0     1.0     1.0   1.0\n",
       "4285     1.0     1.0     0.0     0.0   0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TP.ys.iloc[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 130), (5000, 5))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TP.xs.shape, TP.ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 5])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DLs.one_batch ()[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 0]), torch.Size([1000, 130]), torch.Size([1000, 5]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cat, x_cont, y = DLs.train.one_batch ()\n",
    "x_cat.shape, x_cont.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_LAYERS = [150, 150, 150]\n",
    "# path  = \"../input/jane-fastai-nn5-150-150-150/Jane_nn5_150_150_150\"\n",
    "path  = \"../input/jane-street-market-prediction/Jane_nn5_150_150_150\"\n",
    "featTo5emb_learn = tabular_learner (DLs, layers=HIDDEN_LAYERS, loss_func=nn.BCEWithLogitsLoss (), model_dir='') # '/kaggle/working/')\n",
    "featTo5emb_learn = featTo5emb_learn.load (path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.674614</td>\n",
       "      <td>0.629494</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with valid_loss value: 0.6294940710067749.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_min, lr_steep = featTo5emb_learn.lr_find (start_lr=1e-4, end_lr=0.1, num_it=100)\n",
    "\n",
    "modelfile='Jane_nn5_'+str (HIDDEN_LAYERS).replace (' ', '_').replace (',', '').replace ('[', '').replace (']', '')\n",
    "callbacks = [\n",
    "    EarlyStoppingCallback (monitor='valid_loss', min_delta=0.00001, patience=20),\n",
    "    SaveModelCallback (monitor='valid_loss', fname=modelfile),\n",
    "    ReduceLROnPlateau (monitor='valid_loss', min_delta=0.0001, factor=2.0, min_lr=1e-8, patience=1),\n",
    "    GradientClip (1.0)\n",
    "]\n",
    "\n",
    "epochs  = 1\n",
    "lr      = lr_min\n",
    "featTo5emb_learn.fit_one_cycle (epochs, lr, wd=1e-2, cbs=callbacks)\n",
    "EMB5_MODEL = featTo5emb_learn.model.float ()\n",
    "EMB5_MODEL = EMB5_MODEL.eval ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create time series Dataset for self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 5)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def createLastRecords ():\n",
    "    LAST_RECORDS = [[0.0] * 5]*HIST_LEN\n",
    "    LAST_RECORDS = np.array (LAST_RECORDS)\n",
    "    return LAST_RECORDS\n",
    "\n",
    "LAST_RECORDS = createLastRecords ()\n",
    "LAST_RECORDS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JaneDataset (Datasets):\n",
    "    \n",
    "    def __init__(self, df, tfms=None, n_inp=1):\n",
    "        \n",
    "        super (JaneDataset, self).__init__(df.reset_index (drop=True).iloc[:, range(5)], tfms=tfms, n_inp=n_inp)        \n",
    "        self.labels = None                         #;print ('df.shape =', df.shape)\n",
    "        if df.shape[1] > 5:\n",
    "            self.labels = df.reset_index (drop=True)[R_COLS]    \n",
    "        # print ('df.shape=',df.shape, 'labels.shape=', self.labels.shape)\n",
    "        return\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len (self.items)\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        \n",
    "        global LAST_RECORDS\n",
    "        # if torch.is_tensor (idx):\n",
    "        #     idx = idx.tolist ()\n",
    "        label = None\n",
    "        if self.labels is not None:\n",
    "            label = self.labels.loc[idx].to_numpy ()  #.reshape ((-1,5))\n",
    "        if idx-HIST_LEN+1 >= 0:\n",
    "            idx = range (idx-HIST_LEN+1, idx+1)\n",
    "        else:\n",
    "            idx = range (0, idx+1)\n",
    "        \n",
    "        sample   = None\n",
    "        if isinstance (self.items, pd.DataFrame):\n",
    "            features = np.array (self.items.loc[idx]).reshape ((-1, 5))\n",
    "        else:\n",
    "            features = self.items[idx].reshape ((-1, 5))\n",
    "        \n",
    "        # join with history\n",
    "        if features.shape[0] == HIST_LEN:\n",
    "            LAST_RECORDS = features\n",
    "        else:\n",
    "            LAST_RECORDS = np.vstack ((LAST_RECORDS, features))[-HIST_LEN:]\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            # return LAST_RECORDS.astype ('double'), label.astype ('double')\n",
    "            return LAST_RECORDS, label.astype ('float32')\n",
    "        else:\n",
    "            return LAST_RECORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCustomDLs (df):\n",
    "    \n",
    "    # print ('df.shape =', df.shape)\n",
    "    # features = torch.DoubleTensor (df[F_COLS].to_numpy())\n",
    "    features   = torch.Tensor (df[F_COLS].to_numpy().astype ('float32'))\n",
    "    \n",
    "    labels     = df[R_COLS].astype ('float32')\n",
    "    df         = pd.concat ([pd.DataFrame (EMB5_MODEL (None, features)), labels], axis=1)    #;print ('df.shape =', df.shape)\n",
    "    df.columns = ['logits_1', 'logits_2', 'logits_3', 'logits_4', 'logits_5'] + R_COLS\n",
    "    df_valid   = df.sample (frac = 0.02)\n",
    "    df_train   = df.drop (df_valid.index)\n",
    "    del df\n",
    "    df_valid.reset_index (drop = True, inplace=True)\n",
    "    df_train.reset_index (drop = True, inplace=True)\n",
    "\n",
    "    # pytorch Dataset\n",
    "    tr_dataset = JaneDataset (df_train, n_inp=1)     #;print (tr_dataset[1])\n",
    "    vl_dataset = JaneDataset (df_valid, n_inp=1)     #;print (vl_dataset[1])\n",
    "\n",
    "    tr_datlder = DataLoader (tr_dataset, batch_size=BS)\n",
    "    vl_datlder = DataLoader (vl_dataset, batch_size=BS)  \n",
    "    return tr_datlder, vl_datlder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DL, VAL_DL = getCustomDLs (DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the custom Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.1147, -0.0193, -0.2201, -0.3978, -0.3921]],\n",
       " \n",
       "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.1147, -0.0193, -0.2201, -0.3978, -0.3921],\n",
       "          [ 0.1147, -0.0193, -0.2201, -0.3978, -0.3921],\n",
       "          [ 0.1135, -0.0282, -0.4621, -0.8808, -0.7457]],\n",
       " \n",
       "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.1147, -0.0193, -0.2201, -0.3978, -0.3921],\n",
       "          [ 0.1135, -0.0282, -0.4621, -0.8808, -0.7457],\n",
       "          [-0.2197, -0.2365, -0.3167, -0.3545, -0.3486]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.3135, -0.2858, -0.3958, -0.5926, -0.4414],\n",
       "          [ 0.1381,  0.2782,  0.3833,  0.1895,  0.2560],\n",
       "          [ 0.2740,  0.4207,  0.6774,  0.5357,  0.5923],\n",
       "          ...,\n",
       "          [-3.9332, -5.0434, -7.6149, -7.9102, -7.7101],\n",
       "          [ 0.1215,  0.0253, -0.0428, -0.1339, -0.1460],\n",
       "          [-0.2955, -0.0766,  0.8389,  1.2098,  1.0305]],\n",
       " \n",
       "         [[ 0.1381,  0.2782,  0.3833,  0.1895,  0.2560],\n",
       "          [ 0.2740,  0.4207,  0.6774,  0.5357,  0.5923],\n",
       "          [ 0.1690,  0.1599,  0.1908,  0.2346,  0.2177],\n",
       "          ...,\n",
       "          [ 0.1215,  0.0253, -0.0428, -0.1339, -0.1460],\n",
       "          [-0.2955, -0.0766,  0.8389,  1.2098,  1.0305],\n",
       "          [-1.4471, -1.6748, -1.9631, -2.0124, -2.0135]],\n",
       " \n",
       "         [[ 0.2740,  0.4207,  0.6774,  0.5357,  0.5923],\n",
       "          [ 0.1690,  0.1599,  0.1908,  0.2346,  0.2177],\n",
       "          [-1.4163, -1.7203, -2.2066, -2.3680, -2.2783],\n",
       "          ...,\n",
       "          [-0.2955, -0.0766,  0.8389,  1.2098,  1.0305],\n",
       "          [-1.4471, -1.6748, -1.9631, -2.0124, -2.0135],\n",
       "          [-0.2258, -0.2550, -0.2221, -0.2365, -0.2702]]], dtype=torch.float64),\n",
       " tensor([[0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0.],\n",
       "         [1., 1., 0., 0., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0., 1., 1.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 1.]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DL.one_batch ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 500, 5]), torch.Size([1000, 5]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DL.one_batch ()[0].shape,  TRAIN_DL.one_batch ()[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 500, 5]), torch.Size([100, 5]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VAL_DL.one_batch ()[0].shape,  VAL_DL.one_batch ()[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.1690,  0.1599,  0.1908,  0.2346,  0.2177],\n",
       "          [-1.4163, -1.7203, -2.2066, -2.3680, -2.2783],\n",
       "          [ 0.0573,  0.1005,  0.3653,  0.3411,  0.3582],\n",
       "          ...,\n",
       "          [-1.4471, -1.6748, -1.9631, -2.0124, -2.0135],\n",
       "          [-0.2258, -0.2550, -0.2221, -0.2365, -0.2702],\n",
       "          [ 0.1147, -0.0193, -0.2201, -0.3978, -0.3921]],\n",
       " \n",
       "         [[ 0.0573,  0.1005,  0.3653,  0.3411,  0.3582],\n",
       "          [-2.7502, -3.3928, -5.2118, -5.6234, -5.5497],\n",
       "          [-1.2830, -1.6230, -2.1312, -2.1323, -2.1926],\n",
       "          ...,\n",
       "          [ 0.1147, -0.0193, -0.2201, -0.3978, -0.3921],\n",
       "          [ 0.1147, -0.0193, -0.2201, -0.3978, -0.3921],\n",
       "          [ 0.1135, -0.0282, -0.4621, -0.8808, -0.7457]],\n",
       " \n",
       "         [[-3.1988, -3.9397, -5.9273, -6.1803, -6.1313],\n",
       "          [-0.4592, -0.3984, -0.1925, -0.0342, -0.0385],\n",
       "          [-0.4289, -0.5262, -0.8978, -1.3660, -1.2792],\n",
       "          ...,\n",
       "          [ 0.1147, -0.0193, -0.2201, -0.3978, -0.3921],\n",
       "          [ 0.1135, -0.0282, -0.4621, -0.8808, -0.7457],\n",
       "          [-0.2197, -0.2365, -0.3167, -0.3545, -0.3486]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.3135, -0.2858, -0.3958, -0.5926, -0.4414],\n",
       "          [ 0.1381,  0.2782,  0.3833,  0.1895,  0.2560],\n",
       "          [ 0.2740,  0.4207,  0.6774,  0.5357,  0.5923],\n",
       "          ...,\n",
       "          [-3.9332, -5.0434, -7.6149, -7.9102, -7.7101],\n",
       "          [ 0.1215,  0.0253, -0.0428, -0.1339, -0.1460],\n",
       "          [-0.2955, -0.0766,  0.8389,  1.2098,  1.0305]],\n",
       " \n",
       "         [[ 0.1381,  0.2782,  0.3833,  0.1895,  0.2560],\n",
       "          [ 0.2740,  0.4207,  0.6774,  0.5357,  0.5923],\n",
       "          [ 0.1690,  0.1599,  0.1908,  0.2346,  0.2177],\n",
       "          ...,\n",
       "          [ 0.1215,  0.0253, -0.0428, -0.1339, -0.1460],\n",
       "          [-0.2955, -0.0766,  0.8389,  1.2098,  1.0305],\n",
       "          [-1.4471, -1.6748, -1.9631, -2.0124, -2.0135]],\n",
       " \n",
       "         [[ 0.2740,  0.4207,  0.6774,  0.5357,  0.5923],\n",
       "          [ 0.1690,  0.1599,  0.1908,  0.2346,  0.2177],\n",
       "          [-1.4163, -1.7203, -2.2066, -2.3680, -2.2783],\n",
       "          ...,\n",
       "          [-0.2955, -0.0766,  0.8389,  1.2098,  1.0305],\n",
       "          [-1.4471, -1.6748, -1.9631, -2.0124, -2.0135],\n",
       "          [-0.2258, -0.2550, -0.2221, -0.2365, -0.2702]]]),)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa_dls  = DataLoaders (TRAIN_DL, VAL_DL)\n",
    "sa_dls.one_batch ()\n",
    "sa_dls.train.one_batch () #[:self.dls.train.n_inp]\n",
    "# sa_dls.train.n_inp\n",
    "sa_dls.train.one_batch ()[:sa_dls.train.n_inp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def future_mask (seq_length):\n",
    "    future_mask = np.triu (np.ones ((seq_length, seq_length)), k=1).astype ('bool')\n",
    "    return torch.from_numpy (future_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN (nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_dim, outputCount=0, drop_prob=0.25, nonlin=nn.SiLU ()):\n",
    "        \n",
    "        super(FFN, self).__init__()\n",
    "        \n",
    "        self.nonlin     = nonlin\n",
    "        self.dropout    = nn.Dropout (drop_prob)\n",
    "        self.batchnorm0 = nn.BatchNorm1d (embed_dim)\n",
    "        self.dense1     = nn.Linear (embed_dim, embed_dim)\n",
    "        self.batchnorm1 = nn.BatchNorm1d (embed_dim)\n",
    "        self.dense2     = nn.Linear (embed_dim, embed_dim)\n",
    "        self.batchnorm2 = nn.BatchNorm1d (embed_dim)\n",
    "        self.dense3     = nn.Linear (embed_dim, embed_dim)\n",
    "        self.batchnorm3 = nn.BatchNorm1d (embed_dim)        \n",
    "        self.outDense   = None\n",
    "        if outputCount > 0:\n",
    "            self.outDense   = nn.Linear (embed_dim, outputCount)\n",
    "        self.outActivtn = None\n",
    "        if outputCount == 1 or outputCount == 2:\n",
    "            self.outActivtn = nn.Sigmoid ()\n",
    "        elif outputCount > 0:\n",
    "            self.outActivtn = nn.Softmax (dim=-1)\n",
    "        return\n",
    "\n",
    "    def forward (self, X, **kwargs):\n",
    "        \n",
    "        X = self.dropout (self.batchnorm0 (X))\n",
    "        X = self.dropout (self.nonlin (self.batchnorm1 (self.dense1 (X))))\n",
    "        X = self.dropout (self.nonlin (self.batchnorm2 (self.dense2 (X))))\n",
    "        X = self.dropout (self.nonlin (self.batchnorm3 (self.dense3 (X))))\n",
    "        if self.outActivtn:\n",
    "            X = self.outActivtn (self.outDense (X))\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SANN_Model (nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_dim=5, history_len=HIST_LEN, EMB5_MODEL=EMB5_MODEL):\n",
    "        \n",
    "        super (SANN_Model, self).__init__()\n",
    "        self.history_len   = history_len\n",
    "        self.embed_dim     = embed_dim\n",
    "        self.pos_embedding = nn.Embedding (history_len, 5)        \n",
    "        self.layer_normal  = nn.LayerNorm (embed_dim) \n",
    "        self.multi_att     = nn.MultiheadAttention (embed_dim=embed_dim, num_heads=5, dropout=0.2)\n",
    "        self.ffn           = FFN (embed_dim, outputCount=0)\n",
    "        self.dropout       = nn.Dropout (0.2)\n",
    "        self.layer_normal  = nn.LayerNorm (embed_dim) \n",
    "        self.outDense      = nn.Linear (embed_dim, 5)\n",
    "        # self.outActivtn  = nn.LogSoftmax (dim=1)\n",
    "        # self.criterion   = nn.BCEWithLogitsLoss ()    # nn.NLLLoss ()\n",
    "        self.EMB5_MODEL = EMB5_MODEL\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def forward (self, x):\n",
    "        \"\"\"\n",
    "        Potential methods:\n",
    "            1. query_attn_feat     = self_attn (query_features, past_features, past_features)\n",
    "            2. query_attn_feat_emb = self_attn (query_features, past_features, past_features_emb)\n",
    "            3. query_attn_feat_emb = self_attn (query_features_emb, past_features_emb, past_features_emb)\n",
    "        Now implementing (3.)\n",
    "        \"\"\"\n",
    "        \n",
    "        x    = x.double ()\n",
    "        # x      = x.float ()\n",
    "        # print ('x.shape =', x.shape) \n",
    "        \n",
    "        # q is the last row of x\n",
    "        q      = copy (x[:, -1, :]).view (-1, 1, 5)                  # (bs, 1, 5)\n",
    "        x      = x.view (-1, self.history_len, 5)                    #;print ('x.shape =', x.shape)    # (bs, history_len, 5)\n",
    "                \n",
    "        pos_id = torch.arange (x.size (1)).unsqueeze (0)             #;print ('pos_id.shape   =', pos_id.shape)\n",
    "        pos_x  = self.pos_embedding (pos_id)                         #;print ('pos_x.shape   =', pos_x.shape)\n",
    "        x      = x + pos_x \n",
    "        \n",
    "        x   = x.permute (1, 0, 2)                                     #\"\"\"x: [bs, history_len, embed] => [history_len, bs, embed]\"\"\"   #;print ('x.shape        =', x.shape)\n",
    "        # q   = torch.unsqueeze (q, dim=0)                            #;print ('q.shape =', q.shape) \n",
    "        q   = q.permute (1, 0, 2)                                     #;print ('q.shape        =', q.shape)        \n",
    "        att_mask = future_mask (x.size (0))\n",
    "        \n",
    "        # print ('x.shape =', x.shape) \n",
    "        # print ('q.shape =', q.shape) \n",
    "        # print ('att_mask.shape =', att_mask.shape) \n",
    "        att_output, att_weight = self.multi_att (q, x, x)             #;print ('att_output.shape = ', att_output.shape) \n",
    "        \n",
    "        att_output = self.layer_normal (att_output + q)               #;print ('att_output.shape = ', att_output.shape) \n",
    "        att_output = att_output.permute (1, 0, 2)                     #;print ('att_output.shape = ', att_output.shape)  #;\"\"\"att_output: [s_len, bs, embed] => [bs, s_len, embed]\"\"\" ;print ('att_output.shape = ', att_output.shape) \n",
    "        att_output = att_output.squeeze (dim=1)                       #;print ('att_output.shape = ', att_output.shape) \n",
    "        \n",
    "        x          = self.ffn (att_output)                            #;print ('x.shape = ', x.shape) \n",
    "        x          = self.layer_normal (x + att_output)               #;print ('x.shape = ', x.shape) \n",
    "        out_logits = self.outDense (x)                                #;print ('out_logits.shape = ', out_logits.shape) \n",
    "        return out_logits #.double ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SANN_Model (Input shape: 1000)\n",
       "============================================================================\n",
       "Layer (type)         Output Shape         Param #    Trainable \n",
       "============================================================================\n",
       "                     1000 x 500 x 5      \n",
       "Embedding                                 2500       True      \n",
       "LayerNorm                                 10         True      \n",
       "SiLU                                                           \n",
       "Dropout                                                        \n",
       "BatchNorm1d                               10         True      \n",
       "Linear                                    30         True      \n",
       "BatchNorm1d                               10         True      \n",
       "Linear                                    30         True      \n",
       "BatchNorm1d                               10         True      \n",
       "Linear                                    30         True      \n",
       "BatchNorm1d                               10         True      \n",
       "Linear                                    30         True      \n",
       "____________________________________________________________________________\n",
       "\n",
       "Total params: 2,670\n",
       "Total trainable params: 2,670\n",
       "Total non-trainable params: 0\n",
       "\n",
       "Optimizer used: <function Adam at 0x000000104F170310>\n",
       "Loss function: BCEWithLogitsLoss()\n",
       "\n",
       "Callbacks:\n",
       "  - TrainEvalCallback\n",
       "  - Recorder\n",
       "  - ProgressCallback"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path  = \"../input/jane-fastai-nn5-150-150-150/Jane_nn5_150_150_150\"\n",
    "sa_dls  = DataLoaders (TRAIN_DL, VAL_DL)\n",
    "learn   = Learner (sa_dls, SANN_Model ().double (), loss_func=nn.BCEWithLogitsLoss ()) #, model_dir='/kaggle/working/')\n",
    "# learn = learn.load (path)\n",
    "\n",
    "# learn.save (\"jane-street-market-prediction/trained_model\")\n",
    "# learn = learn.load (\"jane-street-market-prediction/trained_model\")\n",
    "learn.summary ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape = torch.Size([1000, 500, 5])\n",
      "y.shape = torch.Size([1000, 5])\n",
      "pred_logits.shape = torch.Size([1000, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9455, -0.5942,  0.4903, -0.3560,  1.1739],\n",
       "        [ 1.5712,  0.0036,  0.6950, -0.0885,  1.3030],\n",
       "        [ 0.0557, -1.0890,  0.2227, -0.4048,  0.6166],\n",
       "        ...,\n",
       "        [-0.4199, -0.6235,  0.0856,  0.2979, -0.5493],\n",
       "        [ 0.8664, -0.4603,  0.1332, -0.2366,  1.4755],\n",
       "        [-0.2148, -1.1547,  0.0612, -0.3079,  0.3565]], dtype=torch.float64,\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = learn.dls.one_batch ()\n",
    "print ('x.shape =', x.shape)                         #;print ('y.shape =', y.shape) \n",
    "pred_logits = learn.model (x)                        #;print ('pred_logits.shape =', pred_logits.shape) \n",
    "pred_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7668, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_loss = learn.loss_func (pred_logits, y)\n",
    "init_loss.backward ()\n",
    "init_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9455, -0.5942,  0.4903, -0.3560,  1.1739],\n",
       "        [ 1.5712,  0.0036,  0.6950, -0.0885,  1.3030],\n",
       "        [ 0.0557, -1.0890,  0.2227, -0.4048,  0.6166],\n",
       "        ...,\n",
       "        [-0.4199, -0.6235,  0.0856,  0.2979, -0.5493],\n",
       "        [ 0.8664, -0.4603,  0.1332, -0.2366,  1.4755],\n",
       "        [-0.2148, -1.1547,  0.0612, -0.3079,  0.3565]], dtype=torch.float64,\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = learn.model (x)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.00660693421959877, 0.010964781977236271)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_min, lr_steep = learn.lr_find (start_lr=1e-4, end_lr=0.1, num_it=100)\n",
    "lr_min, lr_steep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.749765</td>\n",
       "      <td>0.750642</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with valid_loss value: 0.7506422400474548.\n"
     ]
    }
   ],
   "source": [
    "modelfile = 'Jane_sakt'\n",
    "callbacks = [\n",
    "    EarlyStoppingCallback (monitor='valid_loss', min_delta=0.00001, patience=20),\n",
    "    SaveModelCallback (monitor='valid_loss', fname=modelfile),\n",
    "    ReduceLROnPlateau (monitor='valid_loss', min_delta=0.0001, factor=2.0, min_lr=1e-5, patience=1),\n",
    "    GradientClip (1.0)\n",
    "]\n",
    "\n",
    "epochs  = 1\n",
    "lr      = lr_min\n",
    "learn.fit_one_cycle (epochs, lr, wd=1e-2, cbs=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot_loss (skip_start=0, with_valid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.imports import *\n",
    "from fastai.torch_core import *\n",
    "from fastai.learner import *\n",
    "    \n",
    "@patch\n",
    "@delegates(subplots)\n",
    "def plot_metrics(self: Recorder, nrows=None, ncols=None, figsize=None, **kwargs):\n",
    "    metrics = np.stack(self.values)\n",
    "    names = self.metric_names[1:-1]\n",
    "    n = len(names) - 1\n",
    "    if nrows is None and ncols is None:\n",
    "        nrows = int(math.sqrt(n))\n",
    "        ncols = int(np.ceil(n / nrows))\n",
    "    elif nrows is None: nrows = int(np.ceil(n / ncols))\n",
    "    elif ncols is None: ncols = int(np.ceil(n / nrows))\n",
    "    figsize = figsize or (ncols * 6, nrows * 4)\n",
    "    fig, axs = subplots(nrows, ncols, figsize=figsize, **kwargs)\n",
    "    axs = [ax if i < n else ax.set_axis_off() for i, ax in enumerate(axs.flatten())][:n]\n",
    "    for i, (name, ax) in enumerate(zip(names, [axs[0]] + axs)):\n",
    "        ax.plot(metrics[:, i], color='#1f77b4' if i == 0 else '#ff7f0e', label='valid' if i > 0 else 'train')\n",
    "        ax.set_title(name if i > 1 else 'losses')\n",
    "        ax.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot_metrics ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.recorder.plot_lr ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SANN_Model(\n",
       "  (pos_embedding): Embedding(500, 5)\n",
       "  (layer_normal): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n",
       "  (multi_att): MultiheadAttention(\n",
       "    (out_proj): _LinearWithBias(in_features=5, out_features=5, bias=True)\n",
       "  )\n",
       "  (ffn): FFN(\n",
       "    (nonlin): SiLU()\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (batchnorm0): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dense1): Linear(in_features=5, out_features=5, bias=True)\n",
       "    (batchnorm1): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dense2): Linear(in_features=5, out_features=5, bias=True)\n",
       "    (batchnorm2): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dense3): Linear(in_features=5, out_features=5, bias=True)\n",
       "    (batchnorm3): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (outDense): Linear(in_features=5, out_features=5, bias=True)\n",
       "  (EMB5_MODEL): TabularModel(\n",
       "    (embeds): ModuleList()\n",
       "    (emb_drop): Dropout(p=0.0, inplace=False)\n",
       "    (bn_cont): BatchNorm1d(130, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (layers): Sequential(\n",
       "      (0): LinBnDrop(\n",
       "        (0): BatchNorm1d(130, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Linear(in_features=130, out_features=150, bias=False)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): LinBnDrop(\n",
       "        (0): BatchNorm1d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Linear(in_features=150, out_features=150, bias=False)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): LinBnDrop(\n",
       "        (0): BatchNorm1d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Linear(in_features=150, out_features=150, bias=False)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): LinBnDrop(\n",
       "        (0): Linear(in_features=150, out_features=5, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL = learn.model\n",
    "MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from   torch.autograd import Variable\n",
    "from   torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "DEVICE = torch.device (\"cuda:0\") if torch.cuda.is_available () else torch.device (\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMB5_MODEL = EMB5_MODEL.double ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 5)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LAST_RECORDS = createLastRecords ()\n",
    "LAST_RECORDS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_torch (test_df):\n",
    "    \n",
    "    test_df.drop (columns=['weight', 'date'], inplace=True)\n",
    "    test_df.reset_index (drop=True, inplace=True)                                   #;print (test_df.head())\n",
    "    test_df = torch.DoubleTensor (PIPE.transform (test_df))                         #;print ('test_df.shape =', test_df.shape)\n",
    "    test_df = pd.DataFrame (EMB5_MODEL (None, test_df).detach ().numpy ())\n",
    "    test_df.columns = ['Label_1', 'Label_2', 'Label_3', 'Label_4', 'Label_5', ]     #;print (test_df.head())\n",
    "    test_dataset = JaneDataset (test_df, n_inp=1)\n",
    "    pred = []\n",
    "    for i in range (len (test_dataset)):\n",
    "\n",
    "        batch     = test_dataset[i]\n",
    "        batch     = torch.FloatTensor (batch).view (-1, HIST_LEN, 5)      #;print ('batch.shape =', batch.shape)  #.to (DEVICE).float ()\n",
    "        pred_prob = torch.sigmoid (MODEL (batch)).detach ().cpu ().numpy ()\n",
    "        pred.append (pred_prob)\n",
    "\n",
    "    pred = np.concatenate (pred, axis=0).reshape ((-1, 5))\n",
    "    pred = np.median (pred, axis=1)\n",
    "    return np.where (pred >= 0.5, 1, 0).astype (int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dtype = {\n",
    "    'date'      : 'int64', \n",
    "    'weight'    : 'float32',\n",
    "    'feature_0' : 'float32'\n",
    "}\n",
    "for i in range (1, 130):\n",
    "    k = 'feature_' + str (i)\n",
    "    dtype[k] = 'float32'\n",
    "test_df = pd.read_csv ('../input/jane-street-market-prediction/train.csv.dummy', dtype=dtype)\n",
    "    \n",
    "test_df = test_df.drop (columns=R_COLS+['ts_id'])[:10]\n",
    "predict_torch (test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02371,
     "end_time": "2021-01-22T17:58:55.388369",
     "exception": false,
     "start_time": "2021-01-22T17:58:55.364659",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-22T17:58:55.446325Z",
     "iopub.status.busy": "2021-01-22T17:58:55.445226Z",
     "iopub.status.idle": "2021-01-22T17:58:55.471692Z",
     "shell.execute_reply": "2021-01-22T17:58:55.472274Z"
    },
    "papermill": {
     "duration": 0.058761,
     "end_time": "2021-01-22T17:58:55.472431",
     "exception": false,
     "start_time": "2021-01-22T17:58:55.413670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'janestreet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-a4bb474acd6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mjanestreet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0menv\u001b[0m      \u001b[1;33m=\u001b[0m \u001b[0mjanestreet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_env\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# initialize the environment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0menv_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miter_test\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m        \u001b[1;31m# an iterator which loops over the test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'janestreet'"
     ]
    }
   ],
   "source": [
    "import janestreet\n",
    "env      = janestreet.make_env ()  # initialize the environment\n",
    "env_iter = env.iter_test ()        # an iterator which loops over the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026076,
     "end_time": "2021-01-22T17:58:55.522497",
     "exception": false,
     "start_time": "2021-01-22T17:58:55.496421",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# For direct submission, without using the Trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-22T17:58:55.579270Z",
     "iopub.status.busy": "2021-01-22T17:58:55.577702Z",
     "iopub.status.idle": "2021-01-22T18:03:14.623339Z",
     "shell.execute_reply": "2021-01-22T18:03:14.622620Z"
    },
    "papermill": {
     "duration": 259.074066,
     "end_time": "2021-01-22T18:03:14.623458",
     "exception": false,
     "start_time": "2021-01-22T17:58:55.549392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "threshold = 0.50\n",
    "\n",
    "for test_df, pred_df in env_iter:\n",
    "    if test_df[\"weight\"].item () > 0:\n",
    "        \n",
    "        pred_df.action = predict_torch (test_df)\n",
    "    else:\n",
    "        pred_df.action = 0\n",
    "        \n",
    "    env.predict (pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-22T18:03:14.677426Z",
     "iopub.status.busy": "2021-01-22T18:03:14.676598Z",
     "iopub.status.idle": "2021-01-22T18:03:14.679698Z",
     "shell.execute_reply": "2021-01-22T18:03:14.680217Z"
    },
    "papermill": {
     "duration": 0.032627,
     "end_time": "2021-01-22T18:03:14.680369",
     "exception": false,
     "start_time": "2021-01-22T18:03:14.647742",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print ('Done !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.026572,
     "end_time": "2021-01-22T18:03:14.733653",
     "exception": false,
     "start_time": "2021-01-22T18:03:14.707081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
